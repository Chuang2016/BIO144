\documentclass[english,9pt,aspectraio=169]{beamer}
\usepackage{etex}
\usetheme{uzhneu-en-informal}
%\usepackage{uarial}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\RequirePackage{graphicx,ae}
\usepackage{bm}
\usepackage{fancybox,amssymb,color}
\usepackage{pgfpages}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{animate}
\usepackage{numprint}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{amsmath,natbib}
\usepackage{mathbbol}
\usepackage{babel}
\usepackage{SweaveSlides}
\usepackage{multicol}
\usepackage{xcolor}


\usetheme{uzhneu-en-informal}
\DeclareMathOperator{\po}{Poisson}
\DeclareMathOperator{\G}{Gamma}
\DeclareMathOperator{\Be}{Beta}
\DeclareMathOperator{\logit}{logit}
\def\n{\mathop{\mathcal N}}

\definecolor{Gray}{RGB}{139,137,137}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{Green}{rgb}{0,0.8,0.3}
\definecolor{lightgreen}{rgb}{0,0.7,0.3}
\definecolor{Blue}{rgb}{0,0,1}
\def\myalert{\textcolor{darkred}}
\def\myref{\textcolor{Gray}}
\setbeamercovered{invisible}

\renewcommand{\baselinestretch}{1.2}
\beamertemplateballitem
\DeclareMathOperator{\cn}{cn} % Copy number
\DeclareMathOperator{\ccn}{ccn} % common copy number
\DeclareMathOperator{\p}{p} % common copy number
\DeclareMathOperator{\E}{E} % common copy number
\DeclareMathOperator{\given}{|} % common copy number
\def\given{\,|\,}
\def\na{\tt{NA}}
\def\nin{\noindent}
\pdfpageattr{/Group <</S /Transparency /I true /CS /DeviceRGB>>}
\def\eps{\varepsilon}

\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Wahrscheinlichkeitsma√ü
\def\eps{\varepsilon}
\def\logit{\text{logit}}
%\newcommand{\E}{\mathsf{E}} % Erwartungswert
\newcommand{\Var}{\text{Var}} % Varianz
\newcommand{\NBin}{\text{NBin}}
\newcommand{\Po}{\text{Po}}
\newcommand{\N}{\mathsf{N}}

\newcommand{\hl}{\textcolor{red}}

\newcommand{\ball}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{1ex}{1ex}
\usebeamercolor[fg]{item projected}

{\pgftransformscale{1.75}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%
\usepackage{multicol}
\newcommand{\ballsmall}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{.2ex}{.2ex}

{\pgftransformscale{1}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%



\begin{document}
\SweaveOpts{width=6,height=4}
\fboxsep5pt

\frame{
\title[]{ \centering \Huge Kurs Bio144: \\
Datenanalyse in der Biologie}%\\[.3cm]
\author[Stefanie Muff, Owen L.\ Petchey]{\centering Stefanie Muff  \& Owen L.\ Petchey }
%\institute[]{Institute of Social and Preventive Medicine \\ Institute of Evolutionary Biology and Environmental Studies}
\date[]{Lecture 5: ANOVA \\ 23./24. March 2017}


\maketitle
}


\frame{\frametitle{Overview (todo: check)}
\begin{itemize}
\item One-way ANOVA  
\item Post-hoc tests and contrasts
\item Two-way ANOVA 
\item ANOVA as special cases of a linear model \\[6mm]
\end{itemize}

Note: \\[2mm]
ANOVA = ANalysis Of VAriance  (Varianzanalyse)\\
}


\frame{\frametitle{Course material covered today}

The lecture material of today is based on the following literature:\\[4mm]

\begin{itemize}
\item Chapter 12 from Stahel book ``Statistische Datenenalyse''
%\item ``The new Statistics with R'' chapter 2 (ANOVA) 
\item ``Getting Started with R'' chapters 5.6 and 6.2
\end{itemize}
}

% \frame[containsverbatim]{\frametitle{Recap of the linear regression model}
% to do 
% }


\frame{\frametitle{ANOVA and ANCOVA}
ANOVA = Varianzanalyse\\
ANCOVA = Kovarianzanalyse \\[2mm]

Introduction by Sir R.\ A.\ Fisher (1890-1962). He worked at the agricultural research station in Rothamstead (England). AN(C)OVA are/were therefore traditionally used to analyze agricultural experiments.\\[6mm]

Questions of AN(C)OVA:
\begin{itemize}
\item Generally: Are the means of twor or more groups different?\\[2mm]
\item Example: Are different plant breeds different in important aspects (e.g., yiels / Ertrag)?\\[2mm]
\item Example: What is the influence of different treatments on plants (Biology) or patients (Medicine)? \\[2mm]
\end{itemize}
}


\frame[containsverbatim]{\frametitle{Beispiel: Ertragspotential bei Hybrid-Mais mit erh\"ohter Pilzbrand-Resistenz}
\vspace{-4mm}
{\scriptsize (Source: W. Blanckenhorn, UZH)}\\[2mm]

Es wurden 4 Hybrid-Mais-Sorten angebaut und ihr K\"ornerertrag ermittelt. Jede Sorte wurde an 5 Orten angepflanzt.\\[2mm]

{\bf Frage:} Unterscheiden sich die Hybrid-Mais-Sorten im Ertrag? \\[2mm]
{\scriptsize \myalert{Achtung:} Die Frage bezieht sich auf \emph{irgendeinen} Unterschied. Pr\"aziser k\"onnte man fragen, ob sich irgendeine der Sorten von den anderen unterscheidet?}
\vspace{-5mm}
%
<<dmais,eval=T,echo=F>>=
path <- "../../data_examples/anova/ProjektMaishybriden/"
d.mais <- read.table(paste(path,"MaishybridenDaten.txt",sep=""),header=T)
@
\begin{center}
\setkeys{Gin}{width=0.5\textwidth}
<<fig=TRUE,width=4.0,height=4,echo=F>>=
plot(YIELD ~ as.numeric(HYBRID),d.mais,xlab="Hybrid (Sorte)",ylab="Yield (Ertrag)",xaxt="n")
axis(1,labels=levels(d.mais$HYBRID),at=1:4)
@
\end{center}

ANOVA hilft uns, zu testen, ob es einen Unterschied zwischen den Sorten gibt.
}

\frame{\label{sl:naive}
{\bf Naive idea:} 
To carry out pairwise $t$-tests between any two groups. \\[4mm]

\begin{enumerate}
\item How many tests would this imply?\\[2mm]
\item Why is this not a very clever idea? \\[8mm]
\end{enumerate}

\pause
(Generally, the number of pairwise tests can be calculated by $g(g-1)/2$, where $g$=number of groups.)
}

% \frame{
% {\bf Better idea:} Formulate a model that is able to test whether there is an \myalert{overall difference between the groups.}
% 
% }


\frame[containsverbatim]{
{\bf Better idea} 

Formulate a model that is able to \myalert{test simultaneously} whether there is an \myalert{overall difference between the groups.} That is, ask only {\bf one question!}\\[2mm]

This leads us to the\\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
{\bf Idea of the ANOVA analysis:} \\
Compare the variability within groups ($MS_E$) to the variability between the group means ($MS_G$).
\end{minipage}}
\vspace{-10mm}
\begin{center}
\setkeys{Gin}{width=0.5\textwidth}
<<fig=TRUE,width=4.0,height=4,echo=F>>=
plot(YIELD ~ as.numeric(HYBRID),d.mais,xlab="Hybrid (Sorte)",ylab="Yield (Ertrag)",xaxt="n")
axis(1,labels=levels(d.mais$HYBRID),at=1:4)
d.agg <- aggregate(YIELD~HYBRID,d.mais,FUN=mean)
points(YIELD ~ HYBRID,d.agg,col=2,pch=15)
abline(c(mean(d.mais$YIELD),0),col=2,lwd=2)
text(3.8,63.5,expression(bar(y)),col=2,cex=1.5)
text(1.2,64,expression(bar(y[1 ])),col=2,cex=1.0)
text(2.2,63.3,expression(bar(y[2])),col=2,cex=1.0)
text(3.2,64,expression(bar(y[3])),col=2,cex=1.0)
text(3.8,58,expression(bar(y[4])),col=2,cex=1.0)
@
\end{center}
}

\frame{
We formulate a model as follows:
\begin{eqnarray*}
y_{ij} = \mu_i + e_{ij} \ ,
\end{eqnarray*}
where 
\begin{itemize}
\item $y_{ij}$= ``Ertrag der $j$-ten Pflanze der Sorte $i$''
\item $\mu_i$=``Mittlerer Ertrag der Sorte $i$''
\item $e_{ij}\sim\N(0,\sigma_e^2)$ is an independent error term. \\[4mm]
\end{itemize}

Typically, this is rewritten as
\begin{eqnarray*}
y_{ij} = \mu + \beta_i + e_{ij} \ ,
\end{eqnarray*}
where $\mu + \beta_i = \mu_i$ from above, thus the {\bf group mean} of group $i$.
}

\frame{\frametitle{Single factor ANOVA (Einfaktorielle Varianzanalyse)}
\myalert{More generally}, this leads us to the \myalert{single factor ANOVA:}\\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
Assume we have $g$ groups and in each group $i$ there are $n_i$ measurements of some variable of interest, denoted as $y$. The model is then given as
\begin{eqnarray}
y_{ij} = \mu + \beta_i + e_{ij} \quad \text{for }\quad  i &=&1,\ldots ,g \ , \label{eq:aov}  \\
j &=& 1,\ldots, n_i , \nonumber\\ 
e_{ij} &\sim& \N(0,\sigma_e^2) \quad i.i.d. \nonumber
\end{eqnarray}
\end{minipage}}
~\\[2mm]

\begin{itemize}
\item $\mu$ plays the role of the \myalert{intercept} $\beta_0$ in standard regression models.
\item The estimation of $\mu$, $\beta_2$, \ldots, $\beta_g$ is again done by \myalert{least squares minimization}.
\item The $e_{ij} \sim \N(0,\sigma_e^2) \quad i.i.d.$ assumption is again crucial, so \myalert{model checking} will be needed again.
\end{itemize}

}


\frame{

Attention: Model \eqref{eq:aov} is overparameterized, thus an additional constraint is needed! Most popular:\\[2mm]

\begin{itemize}
\item $\beta_1=0$ (\myalert{treatment contrast}; default in R).
\item $\sum_i\beta_i = 0$ (\myalert{sum-to-zero contrast}).
\end{itemize}
}

\frame{To do: move the actual analysis here, only then continue to explain the F-test etc.!} 

\frame{\frametitle{The ANOVA test: The $F$-test}
Test now \emph{globally} if the groups differ. That is:\\

\begin{eqnarray*}
H_0 &:& \mu_1=\mu_2=\ldots = \mu_g  \quad \text{or, equivalently} \quad  \beta_1=\beta_2 = \ldots = \beta_g   \\[2mm]
H_1 &:& \text{At least two groups are different}
\end{eqnarray*}

~\\[2mm]
To test if more than one parameter in a regression model is =0 at the same time, we have used the {\bf $F$-test} in linear regression -- and we also need the $F$-test here!\\[2mm]

To derive the ingredients of the $F$-test, we again look at the decomposition of variance:
}

\frame{\frametitle{Variance decomposition}
(Remember this idea from week 3, slide 24, and replace $\hat{y_{ij}}$ by $\overline{y}_{\cdot i}$)

\begin{eqnarray*}
SS_{total} &=&  SS_{\text{between groups}} \qquad +\qquad SS_{\text{within groups}} \\
\sum_{i=1}^g \sum_{j=1}^{n_i} (y_{ij}-\overline{y})^2 & = &  \sum_{i=1}^g {n_i(\overline{y}_{\cdot i} - \overline{y})^2} \quad  + \quad
\sum_{i=1}^g \sum_{j=1}^{n_i}  (y_{ij} - \overline{y}_{\cdot i} )^2\\[4mm]
\text{Degrees of freedom:} \\
n-1 &= &   (g-1) \qquad  + \qquad (n-g)
\end{eqnarray*}
~\\


From this:
\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{equation*}
\left.
\begin{array}{c}
MS_G = \frac{SS_{\text{between}}}{g-1} \\[2mm]
MS_E = \frac{SS_{\text{within}}}{n-g}
\end{array}
\right\}
\Rightarrow F = \frac{MS_G}{MS_E} \quad\text{is } \sim F_{g-1,n-g} \text{  distributed.} 
\end{equation*}
\end{minipage}}
}



\frame{\frametitle{Interpretation of the $F$ statistic}
\begin{itemize}
\item $MS_G$: Quantifies the variability {\bf between} groups.
\item $MS_E$: Quantifies the variability {\bf within} groups.\\[4mm]
\end{itemize}

Thus $F = \frac{MS_G}{MS_E}$ is large when $MS_G$ is ``large'' with respect to $MS_E$. The larger $F$, the more likely that $H_0$ is false. \\[4mm]

\begin{itemize}
\item $F$ increases 
\begin{itemize}
\item when the group means become more different, or
\item when the variability within groups decreases.
\end{itemize}
\item On the other hand, $F$decreases
\begin{itemize}
\item when the group means become more similar, or
\item when the variability within groups increases.
\end{itemize}
\end{itemize}

\href{http://onlinestatbook.com/stat_sim/one_way/index.html}
{\beamergotobutton{ANOVA App}}
}


\frame{\frametitle{The ANOVA table}
An overview of the results is typically given in an ANOVA table (Varianzanalyse-Tabelle):\\[6mm]

\begin{tabular}{llllll}
Variation & df & SS & MS = SS/df & F & $p$ \\ 
 \hline
 Between groups & $g-1$ & $SS_G$  &  $MS_G$ & $\frac{MS_G}{MS_E}$ & $\P(F_{g-1,n-g}>|F|))$\\
 Within groups &  $n-g$ & $SS_E$ & $MS_E$ & \\ 
 \hline 
 Total & $n-1$ & $SS_{\text{total}}$ & \\
 \hline 
\end{tabular}
}


\frame[containsverbatim]{\frametitle{Hybrid-Mais example -- Data}
\vspace{-5mm}
 \begin{multicols}{2}
<<echo=FALSE,results=tex>>=
library(xtable)
mais1 <- d.mais[1:10,]
mais2 <- d.mais[11:20,]
ttab1 <- xtable(mais1,digits=0)
ttab2 <- xtable(mais2,digits=0)
print(ttab1, floating = TRUE, include.rownames=FALSE)
print(ttab2,  floating = TRUE, include.rownames=FALSE)
@
 \end{multicols}



% \begin{tabular}{l@{\hspace{-7cm}}l}
% <<dm,eval=T,echo=T>>=
% d.mais[,]
% @
% &
<<>>=
str(d.mais)
@
%\end{tabular}

}


\frame[containsverbatim]{\frametitle{Hybrid-Mais example -- Estimation}\label{sl:r.aov}
Using the \texttt{aov()} function in R directly estimates the ANOVA table:\\[4mm]

<<r.mais,eval=T,echo=T>>=
r.mais <- aov(YIELD ~ HYBRID, d.mais)
summary(r.mais)
@
~\\[4mm]

Here: $F=\Sexpr{format(summary(r.mais)[[1]][[4]][1],nsmall=2,digits=2)}$ is $F$-distributed with 3 and 16 degrees of freedom, and the $p$-value of the test ``$\beta_1=\beta_2=\beta_3=\beta_4$ ?'' is $<0.0001$.\\[4mm]

\colorbox{lightgreen}{\begin{minipage}{10cm}
Conclusion: the four groups are clearly different!
\end{minipage}}

~\\[2mm]
{\bf Exercise:} Look at the table a bit closer. How are \texttt{Df}, \texttt{Sum Sq}, \texttt{Mean Sq}, \texttt{F value} and \texttt{Pr(<F)} related? 
}

\frame[containsverbatim]{
The $F$-distribution with 3 and 16 degrees of freedom, as well as the estimated value $F$=17.68:

\setkeys{Gin}{width=0.6\textwidth}
<<fig=T,width=4,height=4,echo=F>>=
curve(df(x,3,16),0,20,lwd=2)
abline(v=17.7,lwd=2,lty=2)
@
}

\frame{\frametitle{ANOVA as a special case of a linear model}
\colorbox{lightgreen}{\begin{minipage}{10cm}
The clou is: Model \eqref{eq:aov} is identical to the regression model with a factor covariate, see slides 36/37 from week 3. 
\end{minipage}}
~\\[4mm]
{\bf Interpretation: The levels of the factor are now the different group memberships.} \\
Thus (assuming $\beta_1=0$):\\

\begin{equation*}
 y_{ij} = \left\{
\begin{array}{ll}
\mu + e_{ij}, & \text{for group } 1\\
\mu  + \beta_2  + e_{ij}, &  \text{for group } 2\\
...\\
\mu  + \beta_g + e_{ij}, &  \text{for group } g \ ,
\end{array}\right.
\end{equation*} 
~\\[4mm]
and $\hat{y}_{ij} = \overline{y}_{\cdot i} = \mu + \beta_i$ can be interpreted as the predicted value.\\[2mm]



}

\frame[containsverbatim]{
It is therefore possible to perform an ANOVA with the \texttt{lm()} function in R and obtain the same results, either by looking at the summary table:\\[4mm]

<<r.aov.mais>>=
r.lm <- lm(YIELD~HYBRID,d.mais)
summary(r.lm)
@
}

\frame[containsverbatim]{
...or by generating the ANOVA table for an lm object:\\[6mm]

<<>>=
anova(r.lm)
@

~\\[4mm]
Compare the output with the previous slide and slide \ref{sl:r.aov} for a while....
}

\frame[containsverbatim]{\frametitle{Testing modelling assumptions}
Tukey-Anscombe (TA) and QQ plots are useful again:
\setkeys{Gin}{width=0.9\textwidth}
<<TA1,fig=T,echo=F,width=6,height=3.4>>=
par(mfrow=c(1,2))
plot(fitted(r.lm),residuals(r.lm),xlab="Fitted",ylab="Residuals")
abline(h=0,lty=2)
qqnorm(residuals(r.lm))
qqline(residuals(r.lm))
@
}

\frame[containsverbatim]{\frametitle{Exercise: Ern\"ahrung und Blutzucker}
Remember example 3 from the first week: \\[2mm]
24 Personen werden in 4 Gruppen unterteilt. Jede Gruppe erh\"alt eine andere Di\"at \small{(DIAET)}. Es werden zu Beginn und am Ende (nach 2 Wochen) die Blutzuckerwerte gemessen. Die Differenz wird gespeichert \small{(BLUTZUCK)}.\\[2mm]
{\bf Frage:} Unterscheiden sich die Gruppen in der Ver\"anderung der Blutzuckerwerte?\\[3mm]

<<echo=F>>=
path <- "../../data_examples/anova/Blutzucker/"
d.blz <- read.table(paste(path,"blutzucker.dat",sep=""),header=T)
@
\begin{center}
\setkeys{Gin}{width=0.7\textwidth}
<<blz_plot,fig=T,height=4,width=8>>=
par(mfrow=c(1,2))
plot(BLUTZUCK ~ DIAET,d.blz,xaxt="n",main="Streudiagramm")
axis(1,1:4)
boxplot(BLUTZUCK ~ DIAET,d.blz,xaxt="n",xlab="DIAET",main="Boxplot")
axis(1,1:4)
@
\end{center}
}

\frame[containsverbatim]{
Interpret the results and the residual plots:\\[4mm]
<<>>=
d.blz$DIAET <- as.factor(d.blz$DIAET)
summary(aov(BLUTZUCK ~ DIAET,d.blz))
@

\begin{center}
\setkeys{Gin}{width=0.77\textwidth}
<<blz_plot_res,fig=T,height=4,width=8,echo=F>>=
par(mfrow=c(1,2))
r.aov.blz <- aov(BLUTZUCK ~ DIAET,d.blz,xlab="Fitted values", ylab="Residuals")
plot(r.aov.blz$fitted,r.aov.blz$residuals)
abline(h=0,lty=2)
qqnorm(r.aov.blz$residuals)
qqline(r.aov.blz$residuals)
@
\end{center}
}

\frame{\frametitle{Multiple comparisons, multiple tests}
To remember:
\begin{itemize}
\item The $F$-Test is used to check whether \myalert{any two group means} differ. 
\item Using pairwise tests is not a very good idea (see slide \ref{sl:naive}), because this leads to a \myalert{multiple testing problem}: \\[2mm]

 \colorbox{lightgreen}{\begin{minipage}{10cm}
When many tests are carried out, the probability to find a ``significant'' result {\bf by chance} increases.
\end{minipage}}
~\\[6mm]
For instance, for four groups there are $4\cdot 3/2= 6$ pairwise combinations that could be tested. The probability to find \emph{at least one result by chance} is much higher than the 5\% error level!!!\\[6mm]
\end{itemize}

}


\frame{\frametitle{Post-hoc tests}
{\bf Still:} If the test $\beta_1=\beta_2=\ldots= \beta_g=0$ is rejected, one is often interested 
\begin{enumerate}
\item in finding the actual group(s) that deviate(s) from the others.
\item in estimates of the pairwise differences.\\[6mm]
\end{enumerate}

Several methods to circumvent the problem of  too many ``significant'' test results (type-I error) have been proposed. The most prominent ones are:\\[2mm]
\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{itemize}
\item Bonferroni correction
\item Tukey {\bf h}onest {\bf s}ignificant {\bf d}ifferences (HSD) approach
\item Fisher {\bf l}east {\bf s}ignificant {\bf d}ifferences (LSD) approach
\end{itemize}
\end{minipage}}
}

\frame{\frametitle{}
\myalert{\bf Bonferroni correction}\\[2mm]

{\bf Idea:} If a total of $m$ tests are carried out, simply divide the type-I error level $\alpha_0$ (often 5\%) such that

$$\alpha = \alpha_0 / m \ .$$


\myalert{\bf Tukey HSD approach}\\[2mm]

{\bf Idea:} Take into account the distribution of \emph{ranges} (max-min) and design a new test. \\[6mm]

\myalert{\bf Fisher's LSD approach}\\[2mm]

{\bf Idea:} Adjust the idea of a two-sample test, but use a larger variance (namely the pooled variance of all groups).
}

\frame[containsverbatim]{
Calculate the pairwise differences and tests with adjustments for the ``Blutzucker'' example:

<<echo=F>>=
table.bonf <- pairwise.t.test(d.blz$BLUTZUCK,d.blz$DIAET,p.adjust.method= "bonf",pool.sd=T)[[3]]
table.fisher <- pairwise.t.test(d.blz$BLUTZUCK,d.blz$DIAET,p.adjust.method= "none",pool.sd=T)[[3]]
table.tukey <- TukeyHSD(r.aov.blz,conf.level = 0.95)$DIAET
@
\begin{multicols}{2}
Differences:\\
\begin{tabular}{l|ccc}
& 1 & 2 & 3\\
\hline
2 & \Sexpr{table.tukey["2-1","diff"]} &  & \\
3 & \Sexpr{table.tukey["3-1","diff"]} & \Sexpr{table.tukey["3-2","diff"]}\\
4 & \Sexpr{table.tukey["4-1","diff"]} & \Sexpr{round(table.tukey["4-2","diff"],1)} & \Sexpr{format(table.tukey["4-3","diff"],1,1,1)}\\
\end{tabular}

~\\[4mm]
Bonferroni $p$-values: \\
\begin{tabular}{l|ccc}
& 1 & 2 & 3\\
\hline
2 & \Sexpr{round(table.bonf[1,1],2)} &  & \\
3 & \Sexpr{round(table.bonf[2,1],2)}  &  \Sexpr{round(table.bonf[2,2],2)}  \\
4 & \Sexpr{round(table.bonf[3,1],2)}   &\Sexpr{format(table.bonf[3,2],2,2,2)}   & \Sexpr{round(table.bonf[3,3],3)} \\
\end{tabular}

Tukey HSD $p$-values:\\
\begin{tabular}{l|ccc}
& 1 & 2 & 3\\
\hline
2 & \Sexpr{round(table.tukey["2-1","p adj"],2)} &  & \\
3 & \Sexpr{format(table.tukey["3-1","p adj"],2,2,2)} & \Sexpr{round(table.tukey["3-2","p adj"],2)}\\
4 & \Sexpr{round(table.tukey["4-1","p adj"],2)} & \Sexpr{round(table.tukey["4-2","p adj"],2)} & 
\Sexpr{round(table.tukey["4-3","p adj"],3)}\\ 
\end{tabular}

~\\[4mm]
Fisher $p$-values: \\
\begin{tabular}{l|ccc}
& 1 & 2 & 3\\
\hline
2 & \Sexpr{round(table.fisher[1,1],2)} &  & \\
3 & \Sexpr{round(table.fisher[2,1],2)}  &  \Sexpr{round(table.fisher[2,2],3)}  \\
4 & \Sexpr{round(table.fisher[3,1],2)}   &\Sexpr{format(table.fisher[3,2],2,2,2)}   & \Sexpr{round(table.fisher[3,3],4)} \\
\end{tabular}
\end{multicols}

\begin{itemize}
\item Bonferroni $p$-values are the most conservative (largest $p$).
\item Fisher $p$-values are the least conservative (smallest $p$).
\end{itemize}
}

\frame{\frametitle{Other contrasts}
Sometimes additional comparisons are of interest. For example, a new diet is to be compared to other, existing diets. \\[4mm]

In the ``Blutzucker'' example, this could be, for intance: ``Is diet 1 different from diets 2-4?'' \\[4mm]

(Check also chapter 5.6.5 in GSWR, 2nd edition)
}

\frame[containsverbatim]{\frametitle{Two-way ANOVA (Zweiweg-Varianzanalyse)}
Example {\scriptsize(from Hand et al. 1994 / Hothorn/Everitt ``A Handbook of Statistical Analyses Using R'')}: Experiment to study the weight gain of rats, depending on four diets. Protein amounts were either high or low, and the protein source was either beef or cereal. 10 rats for each diet were selected.\\[6mm]

{\bf Question:} How does diet affect weightgain? \\[6mm]

{\bf Complication:} This is a factorial design (gekreuzte Faktoren), because each combination of  protein source (beef/cereal) $\times$ level (high/low) is present (2$\times$2 groups).

Design: 
\begin{center}
\begin{tabular}{l|c|c|}
&beef & cereal \\
\hline
high & group 1 & group 2\\
\hline
low & group 3 & group 4\\
\hline
\end{tabular}
\end{center}
}

\frame[containsverbatim]{\label{sl:mean}
~\\
Start by looking at means and standard deviations in the groups, as well at a graphical description of the means: \\[2mm]
<<echo=F>>=
library(HSAUR3)
@
<<echo=T>>=
tapply(weightgain$weightgain,list(weightgain$source,weightgain$type),FUN=mean)
tapply(weightgain$weightgain,list(weightgain$source,weightgain$type),FUN=sd)
@
~\\[2mm]
\begin{multicols}{2}
\setkeys{Gin}{width=0.5\textwidth}
<<fig=T,width=5,height=4>>=
plot.design(weightgain)
@
\begin{itemize}
\item Protein source (beef/cereal) seems less important than the amount (high/low).
\item Variances seem to be equal in the four groups.
\end{itemize}
\end{multicols}
}


\frame{\frametitle{Two-way ANOVA -- The model}

In the presence of a \myalert{factorial design}, the idea is to add separate effects $\beta_i$ (here $i=1,2$) and $\gamma_j$ (here $j=1,2$) for the $i$th level of the first factor and the $j$th level of the second factor:\\[5mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
Assume we have a factorial design with two factors $\beta_i$ and $\gamma_j$, then the $k$th outcome in the group of $i$ and $j$, $y_{ijk}$ is modelled as
\begin{eqnarray}
y_{ijk} = \mu + \beta_i + \gamma_j +  e_{ijk} \quad \text{with} \quad e_{ijk} &\sim& \N(0,\sigma_e^2) \quad i.i.d. \nonumber
\end{eqnarray}
\end{minipage}}
~\\[5mm]
{\scriptsize
Again, additionl constraints are needed!
\begin{itemize}
\item $\beta_1=\gamma_1=0$ (\myalert{treatment contrast}; default in R).
\item $\sum_i\beta_i = \sum_i\gamma_i = 0$ (\myalert{sum-to-zero contrast}).
\end{itemize}
}
}



\frame[containsverbatim]{\frametitle{Two-way ANOVA in R}
In R, a two-way ANOVA is as simple as one-way ANOVA, just add another variable:\\[4mm]
<<>>=
r.aov <- aov(weightgain ~ source + type,weightgain)
summary(r.aov)
@
~\\[4mm]

Interpretation: There seems to be a difference between low and high amounts of protein. \\[6mm]

However: what if the additive model does not hold?
}


\frame[containsverbatim]{
A so-called \myalert{interaction plot} (\texttt{interaction.plot()} in R) helps to understand if the additive model is reasonable:\\[2mm]

\setkeys{Gin}{width=0.6\textwidth}
<<fig=T,width=6,height=5,echo=F>>=
interaction.plot(x.factor=weightgain$type,
                 trace.factor=weightgain$source,
                 response=weightgain$weightgain,ylab="mean of weightgain",xlab="type",trace.label="Source")
@

The lines are \myalert{not parallel}, indicating that \myalert{there is an interaction} between type and source!\\
{\scriptsize Note: if the additive model $\beta_i + \gamma_j$ holds, the lines would be parallel.}
}


\frame{\frametitle{Two-way ANOVA with interaction}
\begin{itemize}
\item If the purely additive model is not correct, a more general model with an interaction term $(\beta\gamma)_{ij}$ may be used:\\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{eqnarray}
y_{ijk} = \mu + \beta_i + \gamma_j + (\beta\gamma)_{ij} + e_{ijk} \quad \text{with} \quad e_{ijk} &\sim& \N(0,\sigma_e^2) \quad i.i.d. \nonumber
\end{eqnarray}
\end{minipage}}
~\\[4mm]

\item As in linear regression, interactions allow for an \myalert{ interplay between the variables}. 
\item In the rats experiment, increasing the amount from low to high has a different effect in the beef than in the cereal diet.
\item Moreover: The plot on the previous slide shows that for the low amount of proteins case, the cereal diet leads to a larger average weight gain!
\end{itemize}
}


\frame[containsverbatim]{\frametitle{Two-way ANOVA in R -- Including an interaction}

Again the rats example, this time including the interaction term:\\[4mm]

<<>>=
r.aov <- aov(weightgain ~ source * type,weightgain)
summary(r.aov)
@
~\\[4mm]

The coefficient estimates can be obtained as follows:\\[4mm]
<<>>=
r.lm <- lm(weightgain ~ source * type,weightgain)
summary(r.lm)$coef
@
}

\frame{
{\bf Interpretation of the coefficients} 
\myalert{This works in the same way as for categorical covariates in regression!} To see this, let us estimate the means from the model. From the above output, we have {\scriptsize [because of using treatment contrasts]}:\\[4mm]

$\hat\beta_{beef}=0$, $\hat\beta_{cereal}=-14.1$, \\
$\hat\gamma_{high}=0$, $\hat\gamma_{low}=-20.8$,\\
$\hat{(\beta\gamma)}_{cereal/low}=18.8$, $\hat{(\beta\gamma)}_{beef/high}=\hat{(\beta\gamma)}_{beef/low}=\hat(\beta\gamma)_{cereal/high}=0$.\\[4mm]

Therefore:\\[2mm]

\begin{tabular}{l l}
Group 1: beef / high &  $\hat{y}_{beef,high} = 100 + 0 + 0 + 0 = 100$\\
Group 2: cereal / high & $\hat{y}_{cereal,high} = 100 + (-14.1) + 0 + 0 = 85.9$\\
Group 3: beef / low &  $\hat{y}_{beef,low} = 100 + 0 + (-20.8) + 0 = 79.2$\\
Group 4: cereal / low & $\hat{y}_{cereal,low} = 100 + (-14.1) + (-20.8) + 18.8 = 83.9$\\[6mm]
\end{tabular}

Compare these values to slide \ref{sl:mean}!
}

\frame{
And finally, again, checking some modelling assumptions:
\begin{center}
\setkeys{Gin}{width=0.9\textwidth}
<<TA2,fig=T,echo=F,width=6,height=3.4>>=
par(mfrow=c(1,2))
plot(fitted(r.aov),residuals(r.aov),xlab="Fitted",ylab="Residuals")
abline(h=0,lty=2)
qqnorm(residuals(r.aov))
qqline(residuals(r.aov))
@
\end{center}
}


\frame[containsverbatim]{\frametitle{Exercise: }
In an experiment the influence of four levels of fertilizer (DUENGER) on the yield (ERTRAG) on 5 species (SORTE) of crops was investigated. The data contains the following colums:\\
DUENGER (4 levels) \hspace{1cm} SORTE (5 levels) \hspace{1cm} ERTRAG \\[6mm]

<<dmais,eval=T,echo=F>>=
path <- "../../data_examples/WBL/"
d.duenger <- read.table(paste(path,"duenger.dat",sep=""),header=T,sep=",")
@

\begin{multicols}{2}
The first 10 rows of the data:\\[2mm]
<<>>=
d.duenger[1:10,]
@
~\\[6mm]

And the interaction plot:
\setkeys{Gin}{width=0.4\textwidth}
<<fig=T,width=5,height=5,echo=F>>=
interaction.plot(d.duenger$DUENGER,d.duenger$SORTE,d.duenger$ERTRAG)
@
\end{multicols}
}

\frame[containsverbatim]{
The interaction plot indicates that an interaction between SORTE and DUENGER is needed in the analysis. The results and residal plots are given as follows:\\[2mm]

<<>>=
d.duenger$SORTE <- as.factor(d.duenger$SORTE)
d.duenger$DUENGER <- as.factor(d.duenger$DUENGER)
r.duenger <- aov(ERTRAG ~ DUENGER*SORTE,d.duenger)
summary(r.duenger)
@

\setkeys{Gin}{width=0.7\textwidth}
<<fig=T,width=8,height=4,echo=F>>=
par(mfrow=c(1,2))
plot(r.duenger$fitted,r.duenger$residuals,xlab="fitted",ylab="residuals")
abline(h=0,lty=2)
qqnorm(r.duenger$residuals)
qqline(r.duenger$residuals)
@
~\\
Interpretation? What is here the problem (look at the TA plot)? Ideas?
}

\frame[containsverbatim]{
Todo: don't show this slide previously!\\
Log-transform the response (ERTRAG) and repeat the analysis:\\[2mm]

<<>>=
r.duenger2 <- aov(log(ERTRAG) ~ DUENGER*SORTE,d.duenger)
summary(r.duenger2)
@

\setkeys{Gin}{width=1\textwidth}
<<fig=T,width=9,height=3.5,echo=F>>=
par(mfrow=c(1,3))
plot(r.duenger2$fitted,r.duenger2$residuals,xlab="fitted",ylab="residuals")
abline(h=0,lty=2)
qqnorm(r.duenger2$residuals)
qqline(r.duenger2$residuals)
interaction.plot(d.duenger$DUENGER,d.duenger$SORTE,log(d.duenger$ERTRAG))
@
}


\frame{\frametitle{Some remarks }
\begin{itemize}
\item The $t$-test to compare the mean of \myalert{two gropus} is a \myalert{special case of ANOVA}.\\[2mm]
\item ANOVA is a \myalert{special cases of the linear regression model}.\\[2mm]
\item ANOVA is often taught in separate lectures, although it could be integrated in a lecture on linear regression. \\[2mm]
\item ANOVA is traditionally most used to analyze \myalert{experimental data}.\\[2mm]
\end{itemize}
}



\frame{\frametitle{Summary}

}
% \frame{References:
% \bibliographystyle{Chicago}
% \bibliography{refs}
% }



\end{document}

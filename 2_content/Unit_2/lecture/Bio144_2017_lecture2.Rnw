\documentclass[english,9pt,aspectraio=169]{beamer}
\usepackage{etex}
\usetheme{uzhneu-en-informal}
%\usepackage{uarial}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\RequirePackage{graphicx,ae}
\usepackage{bm}
\usepackage{fancybox,amssymb,color}
\usepackage{pgfpages}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{animate}
\usepackage{numprint}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{amsmath,natbib}
\usepackage{mathbbol}
\usepackage{babel}
\usepackage{SweaveSlides}
\usepackage{multicol}
\usepackage{xcolor}


\usetheme{uzhneu-en-informal}
\DeclareMathOperator{\po}{Poisson}
\DeclareMathOperator{\G}{Gamma}
\DeclareMathOperator{\Be}{Beta}
\DeclareMathOperator{\logit}{logit}
\def\n{\mathop{\mathcal N}}

\definecolor{Gray}{RGB}{139,137,137}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{Green}{rgb}{0,0.8,0.3}
\definecolor{lightgreen}{rgb}{0,0.7,0.3}
\definecolor{Blue}{rgb}{0,0,1}
\def\myalert{\textcolor{darkred}}
\def\myref{\textcolor{Gray}}
\setbeamercovered{invisible}

\renewcommand{\baselinestretch}{1.2}
\beamertemplateballitem
\DeclareMathOperator{\cn}{cn} % Copy number
\DeclareMathOperator{\ccn}{ccn} % common copy number
\DeclareMathOperator{\p}{p} % common copy number
\DeclareMathOperator{\E}{E} % common copy number
\DeclareMathOperator{\given}{|} % common copy number
\def\given{\,|\,}
\def\na{\tt{NA}}
\def\nin{\noindent}
\pdfpageattr{/Group <</S /Transparency /I true /CS /DeviceRGB>>}
\def\eps{\varepsilon}

\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Wahrscheinlichkeitsma√ü
\def\eps{\varepsilon}
\def\logit{\text{logit}}
%\newcommand{\E}{\mathsf{E}} % Erwartungswert
\newcommand{\Var}{\text{Var}} % Varianz
\newcommand{\NBin}{\text{NBin}}
\newcommand{\Po}{\text{Po}}
\newcommand{\N}{\mathsf{N}}

\newcommand{\ball}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{1ex}{1ex}
\usebeamercolor[fg]{item projected}

{\pgftransformscale{1.75}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%
\usepackage{multicol}
\newcommand{\ballsmall}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{.2ex}{.2ex}

{\pgftransformscale{1}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%




\begin{document}
\SweaveOpts{width=6,height=4}
\fboxsep5pt

\frame{
\title[]{ \centering \Huge Kurs Bio144: \\
Datenanalyse in der Biologie}%\\[.3cm]
\author[Stefanie Muff, Owen L.\ Petchey]{\centering Stefanie Muff (Lecture) \& Owen L.\ Petchey (Exercises)}
%\institute[]{Institute of Social and Preventive Medicine \\ Institute of Evolutionary Biology and Environmental Studies}
\date[]{Lecture 2: Simple linear regression\\ 2./3. March 2017}


\maketitle
}


\frame{\frametitle{Overview}
\begin{itemize}
\item Introduction of the linear regression model
\item Parameter estimation
\item Simple model checking
\item Goodness of the model: Correlation and $R^2$
\item Tests and confidence intervals
\item Confidence and prediction ranges
\end{itemize}
}



\frame{\frametitle{Course material covered today}
\begin{itemize}
\item Chapter 2 of \emph{Lineare Regression}, p.7-20 (Stahel script),
\item Alternatively, chapters 13.1 - 13.4 in the Stahel book ``Statistische Datenanalyse''.
\end{itemize}
}


\frame[containsverbatim]{\frametitle{The body fat example}
Remember: Aim is to find prognostic factors for body fat, without actually measuring it. \\
Even simpler question: How good is BMI as a predictor for body fat?

\begin{center}

<<read.bodyfat,echo=F,eval=T>>=
path <- "../../data_examples/bodyFat/"
d.bodyfat <- read.table(paste(path,"bodyfat.clean.txt",sep=""),header=T)
d.bodyfat <- d.bodyfat[,c("bodyfat","age","gewicht","hoehe","bmi","neck","abdomen","hip")]
@
\setkeys{Gin}{width=0.5\textwidth}
<<fig=T,width=4,height=4.5>>=
plot(bodyfat ~ bmi,d.bodyfat,xlab="bmi (x)", ylab="body fat (y)")
@
\end{center}

}


\frame[containsverbatim]{\frametitle{Linear relationship}
\begin{itemize}
\item The most simple relationship between an \emph{explanatory variable} ($X$) and a \emph{target/outcome variable} ($Y$) is a linear relationship. All points $(x_i,y_i)$, $i= 1,\ldots, n$, on a  straight line follow the equation
$$y_i = \alpha + \beta x_i\ .$$

\item Here, $\alpha$ is the \myalert{axis intercept} and $\beta$ the \myalert{slope} of the line. 
$\beta$ is also denoted as the regression coefficient of $X$.\\[4mm]


\item If $\alpha=0$ the line goes through the origin.

\item Interpretation of linear dependency: proportional increase in $y$ with increase (decrease) in $x$.
\end{itemize}
}



\frame[containsverbatim]{\frametitle{}

But which is the ``true'' or ``best'' line?
\vspace{-6mm}
\begin{center}
\setkeys{Gin}{width=0.6\textwidth}
<<fig=T,width=4,height=4.5,echo=F>>=
plot(bodyfat ~ bmi,d.bodyfat,xlab="BMI (x)", ylab="bodyfat (y)",xlim=c(15,40))
abline(lm(bodyfat~bmi,d.bodyfat),lwd=2)
abline(c(-25,1.7),col=2,lwd=2)
abline(c(-35,2.1),col=3,lwd=2)
@
\end{center}
}

\frame[containsverbatim]{\frametitle{}
It is obvious that another realization of the data (other 241 males) would lead to a slightly different picture.\\[4mm]

$\Rightarrow$ The model should take this into account! \\[4mm]

{\bf Solution:} Add an \myalert{error term} $e_i$ to the predictor
$$(body fat)_i = \alpha + \beta \cdot bmi_i + e_i  \ ,$$
where $e_i$ treated as a random variable with a \myalert{normally distributied}
$$e_i ~\sim \N(0,\sigma_e^2) \quad \text{for } i = 1,\ldots, n \ .$$

This is a \myalert{model} for $bodyfat$ given $bmi$.
The assumption is that the target value $bodyfat_i$ is the sum of a predicted value ($\alpha + \beta \cdot bmi_i$) plus an error term $e_i$.

}

\frame{\frametitle{The simple linear regression model}
Generally:
\colorbox{lightgray}{\begin{minipage}{10cm}
The linear regression model for the data $\bm{y}=(y_1,\ldots,y_n)$ given $\bm{x}=(x_1,\ldots,x_n)$ is

%
$$y_i = \alpha + \beta x_i + e_i \ , \qquad e_i \sim \N(0,\sigma_e^2) \  \text{independent}.$$
\end{minipage}}
~\\[2mm]
The assumption is that
$$y_i \quad= \quad \underbrace{\text{\ prediction\ }}_{\alpha + \beta x_i} \quad + \quad \underbrace{\text{\ error\ }}_{e_i}$$

\vspace{2mm}
Note:

\begin{itemize}
\item The model for $\bm{y}$ given $\bm{x}$ has \myalert{three parameters}: $\alpha$, $\beta$ and $\sigma_e^2$ .
\item $\bm{x}$ is the \myalert{independent} or \myalert{explanatory} variable.
\item $\bm{y}$ is the \myalert{dependent} or \myalert{outcome} variable. \\
\end{itemize}


}

\frame{\frametitle{}
This is a general approach in statistics:\\[4mm]
\begin{itemize}
\item Formulate a model and modelling assumptions that seem plausible for your data. {\bf A model emerges in our mind}.\\[2mm]
\item Estimate the parameters.\\[2mm]
\item Only now it is a \emph{specific} model.\\[6mm]
\end{itemize}

{\bf Note:} 
\begin{itemize}
\item The linear model propagates the most simple relationship between two variables. When using it, please always think if such a relationship is meaningful/reasonable/plausible.\\[2mm]
\item Always look at the data \alert{before} you start with model fitting.
\end{itemize}
}

\frame{\frametitle{Visualization of regression assumptions}

\includegraphics[width=11cm]{pictures/regrAssumptions.jpg}
}

\frame[containsverbatim]{\frametitle{Insight from data simulation}
{\scriptsize (Simulation are \emph{always} a great way to understand statistics!!)}\\[2mm]

Generate an independent (explanatory) variable $\bm{x}$ and {\bf two} samples of a dependent variable $\bm{y}$ assuming that
$$y_i = 4 - 2x_i + e_i \ , \quad e_i\sim \N(0,0.5^2) \ .$$


\begin{center}
\setkeys{Gin}{width=0.4\textwidth}
<<fig=T,width=3.5,height=3.5,echo=F>>=
set.seed(134539)
par(mar=c(4,4,1,1))
x <- runif(15,-2,2)
y1 <- 4 - 2*x + rnorm(15,0,sd=0.5)
y2 <- 4 - 2*x + rnorm(15,sd=0.5)
plot(x,y1,ylim=c(min(c(y1,y2)),max(c(y1,y2))),ylab="y")
points(x,y2,col=2)
abline(c(4,-2))
legend("topright",legend=c("sample 1","sample 2"),col=1:2, pch=1)
@
\end{center}
Note the different y-coordinates for the two samples.

}


\frame[containsverbatim]{\frametitle{}
Or one larger sample:
\setkeys{Gin}{width=0.6\textwidth}
\begin{center}
<<fig=T,width=4,height=3.8>>=
par(mfrow=c(1,1))
x <- rnorm(100)
y <- 4 - 2*x + rnorm(100,0,sd=0.5)
plot(x,y);abline(c(4,-2))
@
\end{center}

Random variation is always present. This leads us to the next question.
}



\frame{\frametitle{Parameter estimation}
Remember: There are \myalert{three parameters} $\alpha$, $\beta$ and $\sigma_e^2$ that want to be estimated.\\[4mm]

\begin{itemize}
\item {\bf Problem:} For more than two points there is generally no perfectly fitting line.\\[4mm]
\item {\bf Aim:} We want to find the best fitting line.\\[4mm]
\item {\bf Idea:} Minimize the deviations between the points and the line.\\[4mm]
\end{itemize}

But how?

}



\frame{\frametitle{Should we minimize these distances...}

\includegraphics[width=11cm]{pictures/orthRegr.jpg}
}


\frame{\frametitle{... or these?}

\includegraphics[width=11cm]{pictures/regrFitting.jpg}
}


\frame{\frametitle{Least squares}
For multiple reasons (theoretical aspects and mathematical convenience), the parameters are estimated using the \myalert{least squares} approach. In this, the second type of distances are minimized: \\[6mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
The parameters are estimated such that the sum of \myalert{squared vertical distances}

$$\sum_{i=1}^n r_i^2 \ , \qquad r_i = y_i - (\alpha + \beta x_i) $$

is being minimized.
\end{minipage}}

\vspace{9mm}
Note: The vertical deviations $r_i$ are called the \myalert{residuals}. \\[2mm]


}


\frame{\frametitle{Formulas for regression line parameters}

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{eqnarray*}
\hat\beta &=& \frac{\sum_{i=1}^n  (y_i - \overline{y}) (x_i - \overline{x})}{ \sum_{i=1}^n (x_i - \overline{x})^2 } = \frac{cov(\bm{x},\bm{y})}{var(\bm{x})} \\[4mm]
\hat\alpha &=& \overline{y} - \hat\beta \overline{x} \\[4mm]
\hat\sigma_e^2 &=& \frac{1}{n-2}\sum_{i=1}^n R_i^2 \quad \text{with residuals  } R_i = y_i - (\hat\alpha - \hat\beta x_i)
\end{eqnarray*}
\end{minipage}}
~\\[6mm]

The hat on the parameters ($\hat\alpha$,$\hat\beta$,$\hat\sigma_e$) indicates that these are \myalert{estimates}. \\[6mm]

{\small (The derivation of the parameters can be looked up in the Stahel script 2.A b. Idea: Minimization through derivating equations and setting them =0.)}

}



\frame{\frametitle{Do-it-yourself ``by hand''}

Go to the Shiny gallery and try to ``estimate'' the correct parameters.\\[6mm]

You can do this here:\\[2mm]

\url{https://gallery.shinyapps.io/simple_regression/} \\[2mm]


}


\frame[containsverbatim]{\frametitle{Estimation using R}
Let's estimate the regression parameters from the bodyfat example\\[3mm]

%\setkeys{Gin}{width=0.5\textwidth}
<<lmbodyfat,echo=T,eval=T>>=
r.bodyfat <- lm(bodyfat ~ bmi,d.bodyfat)

summary(r.bodyfat)
@

~\\[2mm]
$\Rightarrow$ $\hat\alpha=\Sexpr{round(r.bodyfat$coef[1],2)}$ ,  $\hat\beta=\Sexpr{round(r.bodyfat$coef[2],2)}$, $\hat\sigma_e = \Sexpr{round(summary(r.bodyfat)$sigma,2)}$.
}


\frame[containsverbatim]{
Plotting the resulting line into the scatterplot is simple:\\[4mm]
\begin{center}
\setkeys{Gin}{width=0.5\textwidth}
<<echo=F>>=
library(reporttools)
library(biostatUZH)
@
<<plotbodyfat,fig=T,width=5,height=5.0>>=
plot(bodyfat ~ bmi,d.bodyfat)
abline(r.bodyfat,lwd=2)
@
\end{center}

}

\frame{\frametitle{Are the modelling assumptions met?}
Before we continue to look into the results, we need to \myalert{check if the modelling assumptions are met}!\\[2mm]

Why? Because otherwise we draw invalid conclusions from the results.\\[4mm]

The assumption we took here is that the errors $e_i \sim \N(0,\sigma_e^2)$. This implies four things:\\[2mm]
\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{enumerate}[a)]
\item The expected value of $e_i$ is 0: $\E(e_i)=0$.\\[2mm]
\item All $e_i$ have the same variance: $\Var(e_i)=\sigma_e^2$. \\[2mm]
\item The $e_i$ are normally distributed.\\[2mm]
\item The $e_i$ are independent of each other.
\end{enumerate}
\end{minipage}}

~\\[6mm]
For the moment, we introduce two simple graphical model checking tools:
}

\frame[containsverbatim]{\frametitle{Model checking tool I: Tukey-Anscombe diagram}

The \myalert{Tukey-Anscombe} diagram plots the residuals against the fitted values:
\begin{center}
\setkeys{Gin}{width=0.5\textwidth}
<<TA,fig=T,width=4,height=4,echo=F>>=
plot(r.bodyfat$fitted,r.bodyfat$residuals,xlab="Fitted", ylab="Residuals")
abline(h=0,lty=2)
@
\end{center}
This plot is ideal to check if assumptions a) and b) (and partially d)) are met. Here, this seems fine.
}


\frame[containsverbatim]{\frametitle{Model checking tool II: Histogram of residuals}
 
Look at the histogram of the residuals:
\begin{center}
\setkeys{Gin}{width=0.5\textwidth}
<<hist,fig=T,width=4,height=4,echo=F>>=
hist(r.bodyfat$residuals,nclass=20,xlab="Residuals",main="")
@
\end{center}
The normal distribution assumption (c) seems ok as well.
 
}


%' \frame[containsverbatim]{\frametitle{Model checking tools II: QQ-plot}
%' (Todo: check if Luchsinger has introduced QQ plots)
%' 
%' The quantile-quantile (QQ) plot helps to check if the $e_i$ are normally distributed.
%' \begin{center}
%' \setkeys{Gin}{width=0.5\textwidth}
%' <<QQ,fig=T,width=4,height=4,echo=F>>=
%' qqnorm(r.bodyfat$residuals)
%' qqline(r.bodyfat$residuals)
%' @
%' \end{center}
%'  
%' }




\frame[containsverbatim]{\frametitle{Uncertainty in the estimates $\hat\alpha$ and $\hat\beta$}
Let us look again at the regression output, this time only for the coefficients:\\[6mm]

<<lmbodyfat.uncertainty,echo=T,eval=T>>=
summary(r.bodyfat)$coef
@
~\\[6mm]
The second column shows a standard error of the estimate. The estimates thus seem to contain \myalert{uncertainty}!\\[4mm]

The logical next question is: what is the distribution of the estimates?

}



\frame[containsverbatim]{\frametitle{Distribution of $\hat\alpha$ and $\hat\beta$}
\vspace{-6mm}
Again, a simulation can help to get an idea. We generate data points according to the model
%
$$y_i = 4 - 2x_i + e_i \ , \quad e_i\sim \N(0,0.5^2). $$
In each round, we estimate the parameters and store them:\\[3mm]

<<echo=T, eval=T>>=
niter <- 1000
pars <- matrix(NA,nrow=niter,ncol=2)
for (ii in 1:niter){
x <- rnorm(100)
y <- 4 - 2*x + rnorm(100,0,sd=0.5)
pars[ii,] <- lm(y~x)$coef
}
@
~\\[2mm]
Doing this \numprint{1000} times, we obtain the following distributions for $\hat\alpha$ and $\hat\beta$.
}

\frame[containsverbatim]{\label{sl:dists}
\setkeys{Gin}{width=0.85\textwidth}
<<fig=T,width=8,height=4,echo=T>>=
par(mfrow=c(1,2))
hist(pars[,1],xlab =expression(alpha),main="",freq=F,nclass=20)
hist(pars[,2],xlab=expression(beta),main="",freq=F,nclass=20)
@

This looks suspiciously normal... \\[2mm]
In fact, from theory:
\begin{eqnarray*}
\hat\beta \sim \N(\beta,{\sigma^{(\beta)2}}) & \quad \text{and} \quad & \hat\alpha \sim \N(\alpha,{\sigma^{(\alpha)2}})
\end{eqnarray*}
}

\frame{
The standard deviations ${\sigma^{(\beta)2}}$ and ${\sigma^{(\alpha)2}}$ are defined as

$$\sigma^{(\beta)2} = \sigma_e^2 / \text{SSQ}^{(X)} \qquad \sigma^{(\alpha)2} = \sigma_e^2 \left(\frac{1}{n} + \overline{x}^2/\text{SSQ}^{(X)}\right)$$

with the sum-of-squares for $X$ given as
$$\text{SSQ}^{(X)} = \sum_{i=1}^n (x_i-\overline{x})^2 \ .$$

{\small(See also Stahel 2.2.h)}\\[2mm]
\colorbox{lightgreen}{\begin{minipage}{10cm}
Don't worry, you do not need to know these formulas by heart! 
\end{minipage}}
~\\

\colorbox{lightgray}{\begin{minipage}{10cm}
{\bf You should know that}
\begin{itemize}
\item the parameters estimates $\hat\alpha$ and $\hat\beta$ are \myalert{normally distributed.}
\item the formulas to calculate the variances depend on the residual variance $\sigma_e^2$, the sample size $n$ and SSQ$^{(X)}$.
\end{itemize}
\end{minipage}}
}



\frame[containsverbatim]{\frametitle{How good is the regression model?}
This is, per se, a difficult question.... \\[2mm]

One often considered index is the {\bf coefficient of determination (Bestimmtheitsmass)} $R^2$.
Let us again look at the regression output form the bodyfat example:\\[4mm]

<<lmbodyfat2,echo=T,eval=T>>=
summary(r.bodyfat)$r.squared
@

~\\[0mm]
This is the $R^2$ from the regression of bodyfat against bmi. \\
Compare this to the correlation between the two variables:\\[2mm]

<<lmbodyfat2,echo=T,eval=T>>=
cor(d.bodyfat$bodyfat,d.bodyfat$bmi)
@
~\\
... and square it:\\[2mm]
<<lmbodyfat2,echo=T,eval=T>>=
cor(d.bodyfat$bodyfat,d.bodyfat$bmi)^2
@
}

\frame{

We conclude:\\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
In simple linear regression, $R^2$ is the squared correlation between the independent and the dependent variable.
\end{minipage}}\\[6mm]


Generally, $R^2$ indicates the proportion of variability of the response variable $\bm{y}$ that is explained by the ensemble of all covariates. {\bf The larger $R^2$, the more variability of $\bm{y}$ is captured by the covariates, thus the ``better'' is the model.} (However, we will qualify this statement later in the course...)\\[6mm]

$R^2$ becomes more interesting in \emph{multiple} linear regression.\\

}




\frame{\frametitle{Testing and Confidence Intervals}\label{sl:testsCI}
After the regression parameters and their uncertainties have been estimated, there are typically two fundamental questions to be answered:\\[4mm]

\begin{enumerate}
\item {\bf ''Are the parameters compatible with some specific value?''} \\
Typically, the question is whether the slope $\beta$ might be 0 or not, that is: ``Is there an effect of the covariate $\bm{x}$ or not?''\\
$\Rightarrow$ This leads to a {\bf statistical test}.\\[4mm]

\item {\bf ``Which values of the parameters are compatible with the data?''}\\
$\Rightarrow$ This leads us to determine {\bf confidence intervals}.\\[4mm]
\end{enumerate}
}

\frame[containsverbatim]{
Let's first go back to the output from the bodyfat example:\\[5mm]
<<lmbodyfatTest,echo=T,eval=T>>=
summary(r.bodyfat)$coef
@
~\\[4mm]
Besides the estimate and the standard error (which we discussed before), there is a t value and a probability Pr(>|t|) that we need to understand. \\[2mm]

How do these things help us to answer the two questions above?
}


\frame{\frametitle{Testing the effect of a covariate}
Remember: in a statistical test you first need to specify the \emph{null hypothesis}. Here, typically, the null hypothesis is
$$H_0: \quad \beta = \beta_0 =  0  \ .$$
Included in $H_0$ is the assumption that the data follow the simple linear regression model. \\
{\small (However, you might want to test against another null hypothesis, see Stahel 2.3 a,b).}\\[2mm]

Here, the \emph{alternative hypothesis} is given by
$$H_A: \quad \beta \neq  0  \ ,$$

 }
 
\frame{
Remember: to carry out a statistical test, we need a \emph{test statistic}.\\[4mm]

What is a test statistic?? (Clicker exercise here?)\\[6mm]

\pause

It is some type of summary statistic that follows a known distribution under $H_0$. For our purpose, we use the so-called $T$-statistic\\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{equation}\label{eq:beta}
T=\frac{\hat\beta - \beta_0}{se^{(\beta)}} \quad \text{with} \quad se^{(\beta)}=\sqrt{\hat\sigma_e^2/SSQ^{(X)}}  \ .
\end{equation}
\end{minipage}}

~\\[2mm]
Again: typically, $\beta_0=0$, so the formula simplifies to.... (please think:-))\\[2mm]

Under $H_0$, $T$ has a $t$-distribution with $n-2$ degrees of freedom ($n=$ number of data points).\\
(Question: do you remember why this is a t-distribution? Check Mat183, keyword: t-test.)

}


\frame[containsverbatim]{
So let's again go back to the bodyfat regression output:\\[4mm]

<<lmbodyfatTest,echo=T,eval=T>>=
summary(r.bodyfat)$coef
@
~\\[6mm]

2 tasks:
\begin{enumerate}
\item Please use equation \eqref{eq:beta} to find out how the first three columns (Estimate, Std. Error and t value) are related! Check your ideas by doing some calculations...\\[7mm]
\item Then think how we get the fourth column from the third. \\
Hint: last column contains the {\bf $p$-value} of the test $\beta=0$.\\[2mm]
\end{enumerate}
}

\frame[containsverbatim]{
For task 2 above we can use the built-in ``distribution table'' of the t-distribution in R:\\[2mm]

<<lmbodyfatTest,echo=T,eval=T>>=
2*pt(16.787522,241,lower.tail=F)
@
~\\[6mm]
Conclusion: there is very strong evidence that the BMI is associated with bodyfat, because $p$ is extremely small.\\[4mm]

This basically answers question 1 from slide \ref{sl:testsCI}.\\[6mm]

\textcolor{gray}{\small (Remark: if you forgot the details of the $p$-value, I have a special task for you: go back to the book ``Statistische Datenanalyse'' from W. Stahel and read chapter 8.7.)}
}


\frame[containsverbatim]{\frametitle{A cautionary note on the use of $p$-values}
Maybe you have seen that in statistical testing, often the criterion $p\leq 0.05$ is used to test whether $H_0$ should be rejected. This is often done in a black-or-white manner.\\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
However, we will put a lot of attention to a more reasonable and cautionary interpretation of $p$-values in this course! 
\end{minipage}}
}

% \frame{
% Question: On slide \ref{sl:dists} we have seen that $\hat\beta \sim \N(\beta,{\sigma^{(\beta)2}})$. So why does $T$ in equation \eqref{eq:beta} follow a $t$-distribution, and not a normal distribution? \\[4mm]
% 
% Remember: In Mat183 you have learned that when the variance $\sigma^2_E$ in the formula is replaced by its estimate $\hat\sigma^2_E$, then the normal distribution must be replaced by the $t$-distribution (Keyword: t-test).
% 
% }


\frame{\frametitle{Confidence intervals of regression parameters}
Question 2 from slide \ref{sl:testsCI}: ``Which values of the parameters are compatible with the data?''\\[4mm]

To answer this question, we can determine the confidence intervals of the regression parameters.\\[4mm]

Let us collect the facts we know about $\hat\beta$:
\begin{itemize}
\item $\hat\beta$ is estimated with a standard error of $\sigma^{(\beta)}$.
\item The distribution of $\hat\beta$ is normal, namely $\hat\beta\sim\N(\beta,\sigma^{(\beta)2})$.
\item However, since we need to estimate $\sigma_e^2$ from the data, we have a $t$-distribution.
\end{itemize}
}

\frame[containsverbatim]{
Doing some calculations (blackboard) leads us to the 95\% confidence interval\\[2mm]

\begin{center}
\colorbox{lightgray}{\begin{minipage}{6cm}
$$[\hat\beta - c \cdot \sigma^{(\beta)} ; \hat\beta + c \cdot \sigma^{(\beta)}] \ ,$$
\end{minipage}}
\end{center}
~\\[1mm]

where $c$ is the 97.5\% quantile of the $t$-distribution with $n-2$ degrees of freedom. \\[2mm]

Doing this for the bodfat example ``by hand'' is not hard. We have 241 degrees of freedom:\\[4mm]
<<lmbodyfatCI,echo=T,eval=T>>=
coefs <- summary(r.bodyfat)$coef
beta <- coefs[2,1]
sdbeta <- coefs[2,2] 
beta + c(-1,1) * qt(0.975,241) * sdbeta 
@
~\\[2mm]

}

\frame[containsverbatim]{
Even easier: directly ask R to give you the CIs.\\[2mm]

<<echo=T,eval=T>>=
confint(r.bodyfat,level=c(0.95))
@
~\\[8mm]

In summary,
<<results=tex,echo=F>>=
tableRegression(r.bodyfat)
@
~\\[6mm]
\underline{Interpretation:} for an increase in the bmi by one index point, roughly 1.82\% percentage points more bodyfat are expected, and all true values for $\beta$ between 1.61 and 2.03 are compatible with the observed data. \\[2mm]
}


\frame{\frametitle{Confidence and Prediction Ranges}
\begin{itemize}
\item Remember: When another sample from the same population was taken, the regression line would look slightly different.\\[4mm]
\item There are two questions to be asked:\\[6mm]
\end{itemize}

\begin{enumerate}
\item Which other regression lines are compatible with the observed data?\\[2mm]
$\Rightarrow$ This leads to the {\bf confidence range}. \\[4mm]
\item Where do future observations with a given $x$ coordinate lie? \\[2mm]
$\Rightarrow$ This leads to the {\bf prediction range}.
\end{enumerate}
}


\frame{\frametitle{Bodyfat example}
\vspace{-5mm}
\begin{center}
\setkeys{Gin}{width=0.65\textwidth}
<<confpred,fig=T,eval=T,echo=F,width=5,height=5>>=
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vert[,2],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vert[,3],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vorh[,2],lty=8,lwd=2,col=4)
lines(x=t.xwerte,y=t.vorh[,3],lty=8,lwd=2,col=4)
legend("bottomright", c("confidence range (95%)", "prediction range (95%)"),
lty=8, cex=1,col=c(2,4),lwd=2)
@
\end{center}

Note: The prediction range is much broader than the confidence range.
}


\frame{\frametitle{Calculation of the confidence range}
Given a fixed value of $x$, say $x_0$. The question is: \\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
Where does $\hat y_0 = \hat\alpha + \hat\beta x_0$ lie with a certain confidence (i.e., 95\%)? 
\end{minipage}}\\[2mm]

This question is not trivial, because both $\hat\alpha$ and $\hat\beta$ are estimates from the data and contain uncertainty. \\[2mm]

The details of the calculation are given in Stahel 2.4b. (Idea: $\hat y_0 \pm q\cdot se^{(y_0)}$.) \\[6mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
For \emph{each} $x_0$ one obtains a confidence interval for the expected value $\hat y_0 = \hat\alpha + \hat\beta x_0$. Plotting this interval for all values of $x_0$ one obtains the {\bf confidence range} or {\bf confidence band for the expected values} of $y$.
\end{minipage}}\\[2mm]


Note: For the confidence range, only the uncertainty in the estimates $\hat\alpha$ and $\hat\beta$ are decisive.
}


\frame{
\vspace{-5mm}
\begin{center}
\setkeys{Gin}{width=0.65\textwidth}
<<conf,fig=T,eval=T,echo=F,width=5,height=5>>=
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="Confidence range",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vert[,2],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vert[,3],lty=8,lwd=2,col=2)
legend("bottomright", c("confidence range (95%)"),
lty=8, cex=1,col=c(2),lwd=2)
@
\end{center}

}



\frame{\frametitle{Calculation of the prediction range}
Given a fixed value of $x$, say $x_0$. The question is: \\[6mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
Where does a {\bf future observation} lie with a certain confidence (i.e., 95\%)? 
\end{minipage}}\\[6mm]

To answer this question, we have to consider not only the uncertainty in the predicted value $\hat y_0 =  \hat\alpha + \hat\beta x_0$, but also the error in the equation $e_i \sim \N(0,\sigma_e^2)$. \\[6mm]


This is the reason why the prediction range is always wider than the confidence range.

}


\frame{
\vspace{-5mm}
\begin{center}
\setkeys{Gin}{width=0.65\textwidth}
<<pred,fig=T,eval=T,echo=F,width=5,height=5>>=
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="Prediction range",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vorh[,2],lty=8,lwd=2,col=4)
lines(x=t.xwerte,y=t.vorh[,3],lty=8,lwd=2,col=4)
legend("bottomright", c( "prediction range (95%)"),
lty=8, cex=1,col=c(4),lwd=2)
@
\end{center}

}



\frame{\frametitle{Further reading}

``Points of significance: Simple linear regression'' is a nice, 2-page overview of simple linear regression, summarized in Nature Methods in 2015:\\[4mm]



\url{http://www.nature.com/nmeth/journal/v12/n11/full/nmeth.3627.html}
}

\frame{\frametitle{Summary}
To do 
}

%\frame{References:
%\bibliographystyle{Chicago}
%\bibliography{refs}
%}

\end{document}

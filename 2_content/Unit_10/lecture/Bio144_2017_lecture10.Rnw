\documentclass[english,9pt,aspectraio=169]{beamer}
\usepackage{etex}
\usetheme{uzhneu-en-informal}
%\usepackage{uarial}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\RequirePackage{graphicx,ae}
\usepackage{bm}
\usepackage{fancybox,amssymb,color}
\usepackage{pgfpages}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{animate}
\usepackage{numprint}
\usepackage{vwcol} 
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{amsmath,natbib}
\usepackage{mathbbol}
\usepackage{babel}
\usepackage{SweaveSlides}
\usepackage{multicol}
\usepackage{xcolor}


\usetheme{uzhneu-en-informal}
\DeclareMathOperator{\po}{Poisson}
\DeclareMathOperator{\G}{Gamma}
\DeclareMathOperator{\Be}{Beta}
\DeclareMathOperator{\logit}{logit}
\def\n{\mathop{\mathcal N}}

\definecolor{Gray}{RGB}{139,137,137}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{Green}{rgb}{0,0.8,0.3}
\definecolor{lightgreen}{rgb}{0,0.7,0.3}
\definecolor{Blue}{rgb}{0,0,1}
\def\myalert{\textcolor{darkred}}
\def\myref{\textcolor{Gray}}
\setbeamercovered{invisible}

\renewcommand{\baselinestretch}{1.2}
\beamertemplateballitem
\DeclareMathOperator{\cn}{cn} % Copy number
\DeclareMathOperator{\ccn}{ccn} % common copy number
\DeclareMathOperator{\p}{p} % common copy number
\DeclareMathOperator{\E}{E} % common copy number
\DeclareMathOperator{\Binom}{Binom} % common copy number
\DeclareMathOperator{\given}{|} % common copy number
\def\given{\,|\,}
\def\na{\tt{NA}}
\def\nin{\noindent}
\pdfpageattr{/Group <</S /Transparency /I true /CS /DeviceRGB>>}
\def\eps{\varepsilon}

\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Wahrscheinlichkeitsma√ü
\def\eps{\varepsilon}
\def\logit{\text{logit}}
%\newcommand{\E}{\mathsf{E}} % Erwartungswert
\newcommand{\Var}{\text{Var}} % Varianz
\newcommand{\NBin}{\text{NBin}}
\newcommand{\Po}{\text{Po}}
\newcommand{\N}{\mathsf{N}}
\newcommand{\Bern}{\mathsf{Bern}}
\newcommand{\odds}{\text{odds}}

\newcommand{\hl}{\textcolor{red}}

\newcommand{\ball}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{1ex}{1ex}
\usebeamercolor[fg]{item projected}

{\pgftransformscale{1.75}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%
\usepackage{multicol}
\newcommand{\ballsmall}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{.2ex}{.2ex}

{\pgftransformscale{1}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%



\begin{document}
\SweaveOpts{width=6,height=4}
\fboxsep5pt

\frame{
\title[]{ \centering \Huge Kurs Bio144: \\
Datenanalyse in der Biologie}%\\[.3cm]
\author[Stefanie Muff, Owen L.\ Petchey]{\centering Stefanie Muff  \& Owen L.\ Petchey }
%\institute[]{Institute of Social and Preventive Medicine \\ Institute of Evolutionary Biology and Environmental Studies}
\date[]{Lecture 10: Modelling binary data  \\ 11./12. May 2017}


\maketitle
}


\frame{\frametitle{Overview }
\begin{itemize}
\item Binary response variables
\item Contingency tables, $\chi^2$ test
\item Odds and (log) odds ratios
\item Logistic regression
\item Residual analysis / model checking / deviances
\item Interpretation of the results
\end{itemize}
<<echo=F>>=
library(dplyr)
library(ggplot2)
library(ggfortify)
@
}


\frame{\frametitle{Course material covered today}

The lecture material of today is based on the following literature:\\[4mm]

\begin{itemize}
\item Repetition: Chapter 9.4 about $\chi^2$-Tests in the Luchsinger script \\[4mm]
\item Chapters 9.1 - 9.3 from \emph{The new statistics with R} (Hector book).\\[12mm]
\end{itemize}

Note that I have also uploaded the continuation of the Stahel script, chapters 7-9 that covers GLM models. This is {\bf not} mandatory literature.
}

\frame{\frametitle{Recap of last week: GLMs and Poisson regression}
\begin{itemize}
\item We introduced \alert{generalized linear models} (GLMS) and key terms:\\
\hspace{1cm}{\bf Family} \hspace{1cm}{\bf Linear predictor}  \hspace{1cm}{\bf Link function}\\[4mm]

\item GLMs are useful when the response variable $\bm{y}$ is not continuous \\
($\rightarrow$ residuals are not Gaussian).\\[4mm]

\item Count data usually lead to \alert{Poisson regression}.\\[4mm]
\end{itemize}
}

\frame[containsverbatim]{\frametitle{Introduction}
\begin{itemize}
\item Today, we will look at the case where the \alert{response variable is binary} (0 or 1) or \alert{binomial} (\emph{e.g.} 5 out of 7 trials).\\[2mm]
\item In binary/binomial regression, the question will be: ``Which variables influence the \alert{probability} $p$ of the outcome?''\\[4mm]
\end{itemize}


{\bf Examples:} 
\begin{itemize}
\item Outcome: Heart attack (yes=1, no=0). \\
Question: which variables lead to higher or lower risk of heart attack?\\[2mm]
\item Outcome: Survival (yes=1, no=0).\\
Question: which variables influence the survival probability of premature babies (Fr\"uhgeburten)?\\[2mm]
\end{itemize}
}

\frame{\frametitle{Some repetition: The $\chi^2$ test}
You have dealt with binary (categorical) data in Mat183! Remember the $\chi^2$ test for contingency tables (simplest: 2$\times$2 tables).\\[2mm]

Example: Heart attack and hormonal contraception (Verh\"utungspille) (from Stahel): \\[2mm] %, see Stahel 7.3.j:\\[2mm]

\includegraphics[width=6cm]{graphics/table1.png}\\[2mm]
{\small ``Hormonal contraception'' is the predictor ($x$) and ``heart attack'' the outcome ($y$).}\\[2mm]

{\bf Question:} Does hormonal contraception ($x$) have an influence on heart attacks ($y$)?\\[2mm]

This question is \alert{equivalent to asking whether the proportion} of patients with heart attack \alert{is the same} in both groups.\\



}

\frame[containsverbatim]{
The respective test-statistic can be calculated as\\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{equation*}
T = \sum_{\text{all entries}} \frac{(\text{observed} - \text{expected})^2}{\text{expected}} \ .
\end{equation*}
\end{minipage}}

~\\[8mm]
By hand, $T$ is obtained as 
%
\begin{equation*}
\frac{(23 - 14.8)^2}{14.8} + \frac{(34 - 42.2)^2}{42.2} + \frac{(35 -  43.2)^2}{43.2} + \frac{(132 - 123.8)^2}{123.8} =  8.329
\end{equation*}

~\\[2mm]
and is expected to be $\chi^2_{1}$ distributed {\tiny(one degree of freedom: $(2-1)\cdot(2-1)$)}. \\[3mm]

The $p$-value of this test is given as $\Pr(X\geq 8.329)=\Sexpr{format(1-pchisq(8.329,1),4,4,4)}$. \\[2mm]

<<>>=
pchisq(8.329,1,lower.tail=F)
@
}

\frame[containsverbatim]{
Of course, R can do this for us:\\[2mm]

<<chisq1>>=
contYes <- c(23,34)
contNo <- c(35,132)
data.table <- data.frame(rbind(contYes,contNo))
chisq.test(data.table,correct=FALSE)
@

<<echo=F>>=
#names( data.table) <- c("heartYes","heartNo")
n <- sum(c(contYes,contNo))
TT <- (23 - 58*57/n)^2 /(58*57/n) + (34 - 166*57/n)^2/(166*57/n) + (35 - 58*167/n)^2/(58*167/n) + (132 - 166*167/n)^2/ (166*167/n)
@
~\\[8mm]

Please note: You may sometimes prefer to have \texttt{correct=TRUE} to obtain a better approximation to the $\chi^2$ distribution when expected values in the cells are small (continuity correction of Yates):\\[2mm]

<<chisq2>>=
chisq.test(data.table,correct=TRUE)
@


~\\[4mm]
{\bf In any case, there is \alert{strong evidence} for an association of hormonal contraception with heart attacks!}
}

%\frame{\frametitle{Fisher's exact test}}

\frame{\frametitle{Quantification of a dependency}
%{\tiny(Stahel GLM chapter 7.4)}\\[2mm]

If two variables are not independent, it is often desired to \alert{quantify} the dependency.\\[2mm]

Let one variable be the grouping variable (e.g., hormonal contraception vs no hormonal contraception). Then $\pi_1$ and $\pi_2$ are the relative frequencies (proportions) observed in the two groups. For example:

\begin{eqnarray*}
\pi_1 = 23/57 &=& 0.404 \\
\pi_2 = 35/167 &=& 0.210\\
\end{eqnarray*}

are the proportions of females with a heart attack in the two groups.
}

\frame{

There are at least three numbers that can be calculated to quantify how the two groups differ:\\[2mm]

\begin{itemize}
\item Risk difference: $\pi_1 - \pi_2 = 0.404 - 0.210$ = 0.194\\[4mm]

\item Relative risk: $\pi_1 / \pi_2 = 0.404 / 0.210 = 1.92 $\\[4mm]
\item \alert{Odds ratio} (``Chancenverh\"altnis''):
\begin{equation*}
OR = \frac{\pi_1 / (1-\pi_1)}{\pi_2 / (1-\pi_2)} = \frac{ 0.404 / (1- 0.404)}{0.210 / (1-0.210)} = 2.55 \ ,
\end{equation*}

where $\pi/(1-\pi)$ is the odds (die ``Chance'').\\[4mm]
Interpretation:
\begin{enumerate}
\item   $OR=1$  $\rightarrow$ the two groups are independent.
\item   $OR > 1$ $\rightarrow$ positive dependency.
\item   $OR < 1$ $\rightarrow$ negative dependency.
\end{enumerate}
\end{itemize}
}


\frame{\frametitle{The odds and the odds ratio}
\begin{itemize}
\item The {\bf odds} (``Wetteverh\"altnis''): For a probability $\pi$ the odds is $\pi/(1-\pi)$. For example, if the probability to win a game is 0.75, then the odds is given as 0.75/0.25 or 3:1.\\[4mm]

\item The {\bf odds ratio} is given on the previous slide. It is a ratio of two ratios, or, the {\bf ratio of two odds}.\\[4mm]

\item Often the {\bf log odds ratio} is used:
\begin{equation*}
\log(OR) = \log\left(\frac{\pi_1 / (1-\pi_1)}{\pi_2 / (1-\pi_2)} \right) \ .
\end{equation*}

Why is this simpler? Look at the interpretation:\\[2mm]
\begin{enumerate}
\item   $\log(OR)=0$ $\rightarrow$ the two groups are independent.
\item   $\log(OR)>0$ $\rightarrow$ positive dependency.
\item   $\log(OR)<0$ $\rightarrow$ negative dependency.
\end{enumerate}

\end{itemize}
}

\frame{
Please go to the following webpage for a short klicker exercise:\\
\url{http://www.klicker.uzh.ch/bkx}
}

\frame{\frametitle{Binomial and binary regression}
Usually the situation is more complicated than 
\begin{center}{\bf binary covariate} ($\bm{x}$) $\rightarrow$ {\bf binary outcome} ($\bm{y}$)\\[10mm]\end{center}

Often, we are interested in a relationship 
\begin{center}  {\bf Continuous/categ./binary} variables $\bm{x}^{(1)}$, $\bm{x}^{(2)}$,.. $\rightarrow$ {\bf binary outcome} ($\bm{y}$) \\[10mm]
\end{center} 

$\rightarrow$ A regression model is needed again!


}

\frame[containsverbatim]{\frametitle{Illustrative/working example}
Let us look at an example from chapter 9.2 in Hector (2015): \\[2mm]

Eight groups of beetles were exposed to carbon disulphide (an insecticide) for 5h. For each beetle it was then reported if it was killed or not (1 or 0), but the data were reported in {\bf aggregated} form:\\[6mm]

<<echo=F>>=
library(AICcmodavg)
data(beetle)
beetle
@

~\\[6mm]
{\bf Question:} (How) does the insecticide ($\bm{x}$) affect the survival probability ($\bm{y}$) of the beetles?
}


\frame[containsverbatim]{\frametitle{}
As always, start with a graph:

\begin{center}
\setkeys{Gin}{width=0.6\textwidth}
<<fig=T,width=3.5,height=2.5,echo=F>>=
ggplot(beetle, aes(x = Dose, y = Mortality_rate)) + theme_bw() +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
geom_smooth(span = 1, colour = "red", se = FALSE) +
xlab("Dose") + ylab("Mortality rate")
@
\end{center}

with linear (blue) and smoothed line (red).
}


\frame{\frametitle{}
{\bf What can we see from the plot?}
\begin{itemize}
\item Mortality increases with higher doses of the herbicide (not surprising, right?).\\[2mm]
\item The linear line seems unreasonable. In particular, if one extrapolates to lower or higher doses, mortality would become $<0$ or $>1$, which is not possible. {\tiny (Remember: A probability is between 0 and 1 by definition.)}\\[8mm]
\end{itemize}

{\bf How does one analyze these data correctly?}
\begin{itemize}
\item So far, we know linear and Poisson regression.\\[2mm]
\item Both of these are \alert{not} the correct approaches here.\\[2mm]
\end{itemize}
}

\frame[containsverbatim]{\frametitle{The `wrong' analyses}
{\bf Wrong analysis 1: Linear regression}\\[2mm]

We could simply use 
$$\E(y_i) = \beta_0 + \beta_1 Dose_i $$
with $\E(y_i)=\pi_i =$ probability to die for individuals $i$ with $Dose_i$. \\[2mm]

R does this analysis without complaining (!!):\\ 

<<echo=T, eval=F>>=
lm(Mortality_rate ~ Dose, data=beetle)
@

<<echo=F, eval=T>>=
r.lm <- lm(Mortality_rate ~ Dose, data=beetle)
@

~\\ 
This leads to $\hat\beta_0=\Sexpr{format(r.lm$coef[1],2,2,2)}$ and $\hat\beta_1=\Sexpr{format(r.lm$coef[2],2,2,2)}$. This means for instance that, for a zero dose, the probability to die would be $\E(y_i)=\Sexpr{format(r.lm$coef[1],2,2,2)}$.

~\\
\colorbox{lightgray}{\begin{minipage}{10cm}
{\bf Problems:} 
\begin{itemize}
\item Linear regression leads to impossible predicted probability values! \\
$\Rightarrow$ \alert{Unrealistic predictions!}
\item For $y_i = \beta_0 + \beta_1 Dose_i + e_i$, residuals $e_i$ are {\bf not} normally distributed!
\end{itemize}
\end{minipage}}
}



\frame[containsverbatim]{\frametitle{} \label{sl:poisson}
{\bf Wrong analysis 2: Poisson regression}\\[2mm]

What about Poisson regression with the counts ``\texttt{Number\_killed}'' in the response? We could use

$$\log(\E(y_i)) = \beta_0 + \beta_1 Dose_i$$

with $\E(y_i)=$ number killed. Again, R does this analysis without complaining, although these are not `real' counts:\\ 

<<echo=T, eval=F>>=
glm(Number_killed ~ Dose, data=beetle,family=poisson)
@

<<echo=F, eval=T>>=
r.pois <- glm(Number_killed ~ Dose, data=beetle,family=poisson)
@

~\\[2mm]
This leads to $\hat\beta_0=\Sexpr{format(r.pois$coef[1],2,2,2)}$ and $\hat\beta_1=\Sexpr{format(r.pois$coef[2],2,2,2)}$. \\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
{\bf Problem:} This means for instance that, for a dose of 76, one expects that $\E(y_i)=\exp(\hat\beta_0 + \hat\beta_1 \cdot 76) = 
\Sexpr{format(exp(r.pois$coef[1] + r.pois$coef[2]*76),2,2,2)}$ beetles die. However, there are only around 60 beetles in each group, so the predicted number killed is more than what is available. $\Rightarrow$ \alert{Unrealistic predictions!}
\end{minipage}}
}

\frame{\frametitle{Sidenote: count vs.\ binomial data}
Clarification of the difference between count data and binomial data:\\[4mm]

{\bf Count data:} 
\begin{itemize}
\item Theoretically no upper limit on number of times an ``event'' (e.g., number of birds observed in a forest plot)
\item Counts cannot be expressed as a proportion.\\[6mm]
\end{itemize} %possible to express as a proportion.\\[4mm]

{\bf Binomial data:}
\begin{itemize} 
\item Aggregated version of many binary experiments, that is, each can be 0 or 1. 
\item Therefore, there is an upper limit on the number of times an ``event'' can be observed (e.g., number of deaths cannot be greater than number of living individuals).
\item Successes can be expressed as proportion (number of successes/number of trials).
\end{itemize}
}



\frame{\frametitle{A model for binary data?}
I hope you remember the Bernoulli distribution from Mat183:\\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
The probability distribution of a binary random variable $Y$ $\in \{0,1\}$ with parameter $\pi$ is defined as
\begin{equation*}
\P(Y=1) = \pi \ , \quad  \P(Y=0) = 1-\pi \ .
\end{equation*}
\end{minipage}}

~\\[6mm]
{\bf Characteristics of the Beroulli distribution:}\\ \label{sl:bernoulli}
\begin{itemize}
\item $\E(Y) = \pi =\P(Y=1)$ {\tiny (useful to remember)}\\[2mm]
\item $\Var(Y)=\pi(1-\pi)$.\\[4mm]

$\rightarrow$ The variance of the distribution is determined by its mean.\\[4mm]
\end{itemize}
}


\frame{\frametitle{From binary to binomial data}
Binomial data is an \alert{aggregation of binary data}: \\[2mm]

\begin{itemize}
\item Repeat the experiment with $\P(Y=1)=\pi$ a total number of $n$ times, calculate how often a success was observed ($k$ times).

\item The expected proportion of successes (``success rate'', here $k/n$) has then the same expectation as the success probability of a single experiment: 
\begin{equation*}
\E\left(\frac{\sum_{i=1}^n{Y}}{n}\right) = \pi = \E(Y)  \ .
\end{equation*}
%and the variance of this $\Var\left(\frac{\sum_{i=1}^n{Y}}{n}\right) = \frac{\pi(1-\pi)}{n}$.\\[4mm]
\end{itemize}

\vspace{8mm}
{\small Example: In the beetle data  49 beetles were tested for the lowest dose, of which $k=6$ died, thus $\hat\pi=6/49=0.122$.}
}

\frame{\frametitle{The binomial distribution}

The {\bf binomial distribution} assigns the probability of seeing $k$ success out of $n$ trials, where the success probability of a single trial is $\pi$.\\[2mm] 

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{equation*}
\P(Y = k) = {{n}\choose{k}}\pi^k (1-\pi)^{n-k} \ ,\quad k=0, 1, 2,\ldots, n
\end{equation*}
In short: $$Y \sim  \Binom(n,\pi) \ .$$
\end{minipage}}
~\\[4mm]
{\bf Characteristics of the binomial distribution:}\\ \label{sl:bernoulli}
\begin{itemize}
\item Mean: $\E(Y)=n \cdot \pi$
\item Variance: $\Var(Y)=n\cdot \pi(1-\pi)$\\[2mm]
\end{itemize}

$\rightarrow$ For given $n$, the variance is determined by its mean.\\[4mm]


{\small R functions: \texttt{rbinom(), dbinom()}}
}

\frame{\frametitle{Doing it right: Logistic regression}
We can again use the GLM machinery from last week! The \alert{linear predictor} is as always:

\begin{equation*}
\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ .
\end{equation*}

~\\
We again need a \alert{link function} that relates the linear predictor $\eta_i$ to the expected value $\E(y_i)$. \\[4mm]

Remember we used the $\log$ link last week, but that seems a bad idea here (see slide \ref{sl:poisson}).\\[4mm]

The link function must be chosen such that the expected value $\E(y_i) $ is always between 0 and 1!

}

\frame{\frametitle{Link function: The logistic transformation}
A transformation that assigns a probability ($\pi$) between 0 and 1 a value between $-\infty$ and $\infty$ is the \alert{logit-transformation}:\\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
$$g(\pi) = \log \left(\frac{\pi}{1-\pi}\right) = \log(\pi) - \log(1-\pi) \ .$$
\end{minipage}}

~\\[2mm]
A graph depicts the functional form of $g(\cdot)$: 
\vspace{-6mm}
\begin{center}
\setkeys{Gin}{width=0.45\textwidth}
<<fig=T,echo=F,width=4,height=4>>=
x <- seq(0,1,0.001)
plot(x,log(x/(1-x)),xlab=expression(pi),ylab=expression(log(pi/(1-pi))),cex=0.8)
@
\end{center}
\vspace{-4mm}
{\small See also Box 9.2 (p.\ 123) in \emph{The new statistics with R}.}
}

\frame{\frametitle{The logistic regression model}
\colorbox{lightgray}{\begin{minipage}{10cm}
In order to prevent the expected value $\E(y_i)$ of a binary experiment (0/1) to attain unreasonable values, we thus formulate the \alert{logistic regression model} as
\begin{equation*}
\log\left( \frac{\E(y_i)}{1-\E(y_i)} \right) = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ 
\end{equation*}

with $\E(y_i) = \P(y_i=1) \ .$
\end{minipage}}

~\\[4mm]
\begin{itemize}
\item The \alert{link function} is called the {\bf logistic link}.\\[2mm]
\item The \alert{family} is {\bf binomial}.\\[2mm]
\end{itemize}
}


\frame[containsverbatim]{\frametitle{Doing it right: Fitting a logistic regression}
\begin{itemize}
\item As for the Poisson GLM, we can estimate the parameters $\beta_0, \beta_1, \ldots $ by maximum-likelihood estimation.\\[4mm]
\item Luckily, the \texttt{glm()} function in R can also handle binomial and binary data!\\[4mm]

\item A complication comes from the fact that we need to tell the function \alert{two numbers for the response}:
  \begin{itemize}
  \item The number of successes, encoded as 1 (here: number killed )
  \item The number of failures, encoded as 0 (here: number died)
  \end{itemize}
  \vspace{4mm}
<<>>=
beetle$Number_survived <- beetle$Number_tested - beetle$Number_killed
beetle.glm <- glm(cbind(Number_killed,Number_survived) ~ Dose, 
             data = beetle, family = binomial)
@
\end{itemize}
}

\frame[containsverbatim]{\frametitle{Doing it right: Model diagnostics}
As always, before looking at the regression output, let's do some model diagnostics:

\begin{center}
\setkeys{Gin}{width=0.6\textwidth}
<<fig=T,width=5,height=4.5,echo=F>>=
par(mfrow=c(2,2),mar=c(3,4,2,1))
plot(beetle.glm)
@
\end{center}
}

\frame{
\begin{itemize}
\item As in Poisson regression, it is not clear how do define residuals, there are many ways...\\[3mm]
\item Again, different types of residuals are used in the plots. \\[3mm]
\item \alert{Be careful}: such plots are only reasonable for \alert{aggregated data} (which we have here)! The larger the groups, the more precise are the underlying assumptions (approximate equality of distributions).\\[3mm]
\item See example on slide \ref{sl:binary} for an example with non-aggregated (binary) data.
\end{itemize}
}


\frame[containsverbatim]{\frametitle{Doing it right: Interpreting the coefficients}
Let's look at the coefficients:\\[2mm]

<<>>=
summary(beetle.glm)$coef
@
~\\[4mm]

The intercept and slope are estimated as  
\begin{equation*}
\hat\beta_0 = \Sexpr{format(summary(beetle.glm)$coef[1,1],3,3,3)} \quad \text{and}\quad \hat\beta_1 = \Sexpr{format(summary(beetle.glm)$coef[2,1],3,3,3)} \ ,
\end{equation*}

with standard errors and $p$-values. Very clearly, the dose influences the survival probability ($p<<0.001$), and $\hat\beta_1 >0$, thus, \alert{the larger the dose, the larger the mortality probability} (positive relation; {\scriptsize be careful, this is wrong in the Hector book!!}). \\[2mm]

This is a {\bf qualitative interpretation} of the coefficients.\\[4mm]

{\small Note: The $\beta$ coefficients are approximately normally distributed as $\N(\hat\beta,\hat\sigma^2_\beta)$. }\\
{\small $\rightarrow$ confidence intervals etc.\ can be calculated as in the linear case!}
}


\frame{\frametitle{Quantitative interpretation of the coefficients}
Remember the regression model
\begin{equation*}
\log\left( \frac{\E(y_i)}{1-\E(y_i)} \right) = \beta_0 + \beta_1 Dose_i \ . 
\end{equation*}

~\\
To understand what $\beta_1$ tells us, let's transform the equation such that $\E(y_i)=\P(y_i=1)$ is on the left. Some algebra leads to 

\begin{equation}\label{eq:prob}
\P(y_i=1 \given Dose_i)= \frac{\exp(\beta_0 + \beta_1 Dose_i)}{1 + \exp(\beta_0 + \beta_1 Dose_i)} \ . 
\end{equation}

~\\[2mm]
\colorbox{lightgray}{\begin{minipage}{10cm}
It is then possible to calculate the \alert{odds} (``Chance'') 
\begin{equation*}
\odds(y_i=1 \given Dose_i) = \frac{\P(y_i=1 \given Dose_i)}{\P(y_i=0\given Dose_i)}= \exp(\beta_0 + \beta_1 Dose_i)\ . 
\end{equation*}
\end{minipage}}
}

\frame{
If the $Dose_i$ is then increased by 1 unit in concentration (from $x$ to $x+1$), the \alert{odds ratio} is given as

\begin{equation*}
\frac{\odds(y_i=1 \given Dose_i = x + 1)}{\odds(y_i=1 \given Dose_i = x)} = \exp{(\beta_1)} = \exp(\Sexpr{format(summary(beetle.glm)$coef[2,1],3,3,3)}) = \Sexpr{format(exp(summary(beetle.glm)$coef[2,1]),2,2,2)} \ .
\end{equation*}
~\\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
{\bf Interpretation:} When the dose is increased by 1 unit, the odds to die increases by a factor of $\Sexpr{format(exp(summary(beetle.glm)$coef[2,1]),2,2,2)}$.
\end{minipage}}
\vspace{6mm}

\colorbox{lightgray}{\begin{minipage}{10cm}
Moreover, taking the $\log$ on the above equation shows that {\bf $\beta_1$ can be interpreted as a log odds ratio}:
\begin{equation*}
\beta_1 = \log\left( \frac{\odds(y_i=1 \given Dose_i = x + 1)}{\odds(y_i=1 \given Dose_i = x)}\right)
\end{equation*}
\end{minipage}}
}

\frame[containsverbatim]{\frametitle{Doing it right: The \texttt{anova()} table}
We can look at the \alert{Analysis of Deviance} table (directly using \texttt{test="Chisq"}):\\[4mm]

<<>>=
anova(beetle.glm,test="Chisq")
@

~\\[4mm]
{\bf Interpretation:} The total deviance is 267.66, and of this 259.23 is explained by \texttt{Dose} (using 1 degree of freedom). This seems really good, because the $\chi^2$ test gives a $p$-value that is reeeeallly small ($<2.2e-16$).
}



\frame[containsverbatim]{\frametitle{Plotting the fit}

A fitted curve can be added to the raw data by plotting $\P(y_i=1)$ against the Dose, using equation (\ref{eq:prob}):

\begin{center}
\setkeys{Gin}{width=0.6\textwidth}
<<fig=T,width=3.5,height=2.5,echo=F>>=
xx <- seq(45,80,0.01)
eta <- -14.578 + 0.2455*xx
ppi <- exp(eta)/(1+exp(eta))
dd <- data.frame(xx=xx,ppi=ppi)
ggplot(beetle, aes(x = Dose, y = Mortality_rate)) + theme_bw() +
geom_point() +
xlab("Dose") + ylab("Mortality rate (Pr(Y=1))")  +
geom_line(aes(xx, ppi,colour = "red"), dd, size=1.2) + 
  scale_color_discrete(guide = FALSE)
@
\end{center}

(Compare to Figure 9.1 in the Hector book \emph{The new statistics with R}.)
}

\frame{\frametitle{Overdispersion}
Remember :
\begin{itemize}
\item Slide \ref{sl:bernoulli}: $\E(Y)=\pi$ and $\Var(Y)=\pi(1-\pi)$, thus \alert{the variance is determined by the mean!} \\[2mm]

\item ``Overdispersion'' means \alert{``extra variability''} (larger than the model predicts). \\[2mm]
\item Reason: Variables are missing in the model!\\[2mm]
\item Overdispersion leads to \alert{too small $p$-values}.\\[2mm]
\item Detectable by looking at the \alert{residual deviance}: \\[2mm]
\texttt{Residual deviance} $>>$ \texttt{df} $\quad\rightarrow$ Overdispersion \\[2mm]

\item Also possible: underdispersion (dependency in the data), if:\\[2mm]
\texttt{Residual deviance} $<<$ \texttt{df}  
\end{itemize}
}

\frame[containsverbatim]{
Here, the residual deviance is {\bf 8.44} with {\bf 6} degrees of freedom. Is this good or bad?\\[2mm]

<<>>=
1-pchisq(8.438,6)
@
~\\
$\rightarrow$ $p=\Sexpr{format(1-pchisq(8.438,6),2,2,2)}$ seems not problematic.\\[6mm]

One can nevertheless account for overdispersion by switching to a \alert{\texttt{quasibinomial}} model. This allows to estimate the dispersion parameter separately.\\[4mm]
}


\frame[containsverbatim]{


<<>>=
beetle.glm2 <- glm(cbind(Number_killed,Number_survived) ~ Dose, 
             data = beetle, family = quasibinomial)
summary(beetle.glm2)
@
}





\frame{\frametitle{Binary response / non-aggregated data}
\begin{itemize}
\item In the beetle example, we were in a comfortable situation: For each level of the does, we had several beetles. For instance, 55 beetles at lowest dose (49.06), of which 6 died (1) and 49 survived (0). This was \alert{binomial} data, an aggregated version of 55 trials with 0 or 1 outcome.\\[3mm]
\item In reality, one often has only one trial (0/1) for a (combination of) covariate(s).\\[3mm]
\item The analysis is the same as for aggregated data, however there are a few complications with graphical descriptions and model checking. \\[6mm]
\end{itemize}

{\bf Example}: Blood screening {\tiny (see week 1; data from Hothorn \& Everitt 2014, chapter 7.3)}
}

\frame[containsverbatim]{\frametitle{Blood screening example}
<<>>=
library(HSAUR3)
data("plasma",package="HSAUR3")
plasma$y <- as.integer(plasma$ESR)-1
@
 
{\small
\begin{multicols}{2}
<<echo=FALSE,results=tex>>=
library(xtable)
pol1 <- plasma[1:16,]
pol2 <- plasma[17:32,]
ttab1 <- xtable(pol1,display=c("s","f","d","s","d"))
ttab2 <- xtable(pol2,display=c("s","f","d","s","d"))
print(ttab1, floating = TRUE, include.rownames=FALSE)
print(ttab2,  floating = TRUE, include.rownames=FALSE)
@
\end{multicols}
}
}

\frame{\frametitle{}
{\bf Question:} Is a high ESR (erythrocyte sedimentation rate) an indicator for certain diseases (rheumatic disease, chronic inflammations)?\\[3mm]

{\bf  Specifically: } Are the concentrations of the plasma proteins Fibrinogen and Globulin (which are disease indicators) association between ESR level ESR$<20mm/hr$ and the ?\\[6mm]

{\bf The model to be fitted:}\\

\begin{equation*}
\log\left( \frac{\E(y_i)}{1-\E(y_i)} \right) = \beta_0 + \beta_1 fibrinogen_i + \beta_2 globulin_i \ ,
\end{equation*}
\\[4mm]
with $\E(y_i)=\P(y_i=1)=\pi_i$. \\
{\tiny Equivalently: $y_i \sim \Bern(\pi_i)$}
}

\frame[containsverbatim]{\frametitle{Complication 1 with binary data: Graphical description}
Plotting the response $y$ (indictor for ESR$>20$) against the covariats does not lead to very illustrative graphs:
\vspace{-6mm}

\begin{center}
\setkeys{Gin}{width=1\textwidth}
<<fig=T,echo=F,width=6,height=3.5>>=
par(mfrow=c(1,2))
plot(y ~ fibrinogen,plasma,ylab="ESR>20")
plot(y ~ globulin,plasma,ylab="ESR>20")
@
\end{center}
}

\frame[containsverbatim]{
It is a bit more illustrative to give a \alert{conditional density plot} (\texttt{cdplot()}): 
\vspace{-2mm}

\begin{center}
\setkeys{Gin}{width=0.9\textwidth}
<<cdplot_ESR,fig=T,width=7,height=4,echo=F>>=
par(mfrow=c(1,2))
cdplot(ESR ~ fibrinogen,plasma)
cdplot(ESR ~ globulin,plasma)
@
\end{center}
}

% \frame[containsverbatim]{
%  
% 
% \begin{center}
% \setkeys{Gin}{width=0.9\textwidth}
% <<cdplot_ESR,fig=T,width=7,height=4,echo=F>>=
% par(mfrow=c(1,2))
% qplot(fibrinogen,y,data=plasma,geom=c("point","smooth")) + theme_bw()
% @
% \end{center}
% }


\frame[containsverbatim]{\frametitle{Complication 2: Model diagnostics}\label{sl:binary}
{\bf a) Residuals plots:}\\[2mm]

\alert{Plotting the residuals} is possible, but \alert{not meaningful}. \\
{\scriptsize Why? Becasue the model checking assumptions rely on aggregated data!}

\begin{center}
\setkeys{Gin}{width=0.6\textwidth}
<<fig=T,width=5,height=4.5,echo=F>>=
plasma.glm <- glm(y ~ fibrinogen + globulin, data = plasma, family=binomial)
par(mfrow=c(2,2),mar=c(3,4,2,1))
plot(plasma.glm)
@
\end{center}
}

\frame{\frametitle{}
{\bf b) Residual deviance:}\\[4mm]

For non-aggregated data, the \texttt{residual deviance} vs.\ \texttt{df} relation {\bf cannot be used to detect overdispersion}s!!
}
 
 
\frame[containsverbatim]{\frametitle{Your turn!}
Apart from the above complications, fitting and interpreting the model is analogous to aggregated binary data. Let's continue with the blood screening example:\\

<<echo=T,eval=F>>=
plasma.glm <- glm(y ~ fibrinogen + globulin, data = plasma, family = binomial)
@

~\\[2mm]
Please look at the model outcomes (summary and anova table) on the next slides and answer the following questions:

\begin{enumerate}
\item Is there an an effect of figrinogen and/or globulin on the outcome (ESR$>20$)?\\[2mm]
\item What is the \emph{quantitative} interpretation of the $\beta_1$ coefficient (what happens to $\P(ESR>20)$ when fibrinogen incrases by 1 unit)? \\[2mm]
\item Is a \texttt{quaisbinomial} model more suitable for these data?
\end{enumerate}
}

\frame[containsverbatim]{
<<>>=
summary(plasma.glm)
@
}

\frame[containsverbatim]{
<<>>=
anova(plasma.glm,test="Chisq")
@
}


% \frame[containsverbatim]{\frametitle{Your turn!}
% 
% <<>>=
% path <- "../../data_examples/WBL/"
% d.heart <- read.table(paste(path,"heart.dat",sep=""),header=T,sep=",")
% @
% }
% 
% \frame[containsverbatim]{
% <<echo=F>>=
% names(data.table)<- c("heartYes","heartNo")
% data.table$cont <- c(1,0)
% @
% 
% <<>>=
% data.table
% @
% 
% ~\\[2mm]
% <<>>=
% r.heart.glm <- glm(cbind(heartYes,heartNo) ~ cont,data.table,family=binomial)
% summary(r.heart.glm)
% @
% 
% }


\frame{\frametitle{Summary}
\begin{itemize}
\item Logistic regression is useful to model binary/binomial data.\\[3mm]
\item The link function is the logistic link.\\[3mm]
\item The coefficients of logistic regression are log odds ratios \\
\begin{center}$\Leftrightarrow \quad$ $\exp(\beta)$ is an odds ratio \\[2mm]\end{center}
\item Differences between aggregated (binomial) and binary data.\\[3mm]
\item Overdispersion not detectable for binary data!
\end{itemize}
}
% \frame{References:
% \bibliographystyle{Chicago}
% \bibliography{refs}
% }



\end{document}

\documentclass[english,9pt,aspectraio=169]{beamer}
\usepackage{etex}
\usetheme{uzhneu-en-informal}
%\usepackage{uarial}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\RequirePackage{graphicx,ae}
\usepackage{bm}
\usepackage{fancybox,amssymb,color}
\usepackage{pgfpages}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{animate}
\usepackage{numprint}
\usepackage{vwcol} 
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{amsmath,natbib}
\usepackage{mathbbol}
\usepackage{babel}
\usepackage{SweaveSlides}
\usepackage{multicol}
\usepackage{xcolor}


\usetheme{uzhneu-en-informal}
\DeclareMathOperator{\po}{Poisson}
\DeclareMathOperator{\G}{Gamma}
\DeclareMathOperator{\Be}{Beta}
\DeclareMathOperator{\logit}{logit}
\def\n{\mathop{\mathcal N}}

\definecolor{Gray}{RGB}{139,137,137}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{Green}{rgb}{0,0.8,0.3}
\definecolor{lightgreen}{rgb}{0,0.7,0.3}
\definecolor{Blue}{rgb}{0,0,1}
\def\myalert{\textcolor{darkred}}
\def\myref{\textcolor{Gray}}
\setbeamercovered{invisible}

\renewcommand{\baselinestretch}{1.2}
\beamertemplateballitem
\DeclareMathOperator{\cn}{cn} % Copy number
\DeclareMathOperator{\ccn}{ccn} % common copy number
\DeclareMathOperator{\p}{p} % common copy number
\DeclareMathOperator{\E}{E} % common copy number
\DeclareMathOperator{\given}{|} % common copy number
\def\given{\,|\,}
\def\na{\tt{NA}}
\def\nin{\noindent}
\pdfpageattr{/Group <</S /Transparency /I true /CS /DeviceRGB>>}
\def\eps{\varepsilon}

\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Wahrscheinlichkeitsma√ü
\def\eps{\varepsilon}
\def\logit{\text{logit}}
%\newcommand{\E}{\mathsf{E}} % Erwartungswert
\newcommand{\Var}{\text{Var}} % Varianz
\newcommand{\NBin}{\text{NBin}}
\newcommand{\Po}{\text{Po}}
\newcommand{\N}{\mathsf{N}}

\newcommand{\hl}{\textcolor{red}}

\newcommand{\ball}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{1ex}{1ex}
\usebeamercolor[fg]{item projected}

{\pgftransformscale{1.75}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%
\usepackage{multicol}
\newcommand{\ballsmall}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{.2ex}{.2ex}

{\pgftransformscale{1}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%



\begin{document}
\SweaveOpts{width=6,height=4}
\fboxsep5pt

\frame{
\title[]{ \centering \Huge Kurs Bio144: \\
Datenanalyse in der Biologie}%\\[.3cm]
\author[Stefanie Muff, Owen L.\ Petchey]{\centering Stefanie Muff  \& Owen L.\ Petchey }
%\institute[]{Institute of Social and Preventive Medicine \\ Institute of Evolutionary Biology and Environmental Studies}
\date[]{Week 11: Modelling binary data  \\ 11./12. May 2017}


\maketitle
}


\frame{\frametitle{Overview (todo: check)}
\begin{itemize}
\item Binary response variables
\item Contingency tables, $\chi^2$ test
\item Odds and (log) odds ratios
\item Logistic regression
\item Residual analysis / model checking / deviances
\item Interpretation of the results
\end{itemize}
<<echo=F>>=
library(dplyr)
library(ggplot2)
library(ggfortify)
@
}


\frame{\frametitle{Course material covered today}
\begin{itemize}
\item Repetition: Chapter 10 in the Stahel book from last semester.\\[4mm]
\item (Ev? Stahel GLM Script, (parts of) chapters 7 and 8) \\[4mm]
\item Chapters 9.1 - 9.3 from ``The new statistics with R''.
\end{itemize}
}

\frame{\frametitle{Recap of last week: GLMs and Poisson regression}
\begin{itemize}
\item We introduced \alert{generalized linear models} (GLMS) and key terms:\\
\hspace{1cm}{\bf Family} \hspace{1cm}{\bf Linear predictor}  \hspace{1cm}{\bf Link function}\\[4mm]

\item GLMs are useful when the response variable $\bm{y}$ is not continuous \\
($\rightarrow$ residuals are not Gaussian).\\[4mm]

\item Count data usually lead to \alert{Poisson regression}.\\[4mm]
\item However, for count data it may sometimes be ok to use linear regression with $\log(\bm{y})$ in the response.
\end{itemize}
}

\frame[containsverbatim]{\frametitle{Introduction}
\begin{itemize}
\item Today, we will look at the case where the \alert{response variable is binary} (0 or 1) or \alert{binomial} (\emph{e.g.} 5 out of 7 trials).\\[2mm]
\item In binary/binomial regression, the question will be: ``Which variables influence the \alert{probability} $p$ of the outcome?''\\[4mm]
\end{itemize}


{\bf Examples:} 
\begin{itemize}
\item Outcome: Heart attack (yes=1, no=0). \\
Question: which variables lead to higher or lower risk of heart attack?\\[2mm]
\item Outcome: Survival (yes=1, no=0).\\
Question: which variables influence the survival probability of premature babies (Fr\"uhgeburten)?\\[2mm]
\end{itemize}
}

\frame{\frametitle{Some repetition: The $\chi^2$ test}
You have dealt with binary (categorical) data in Mat183! Remember the $\chi^2$ test...\\[2mm]

Example: Heart attack and hormonal contraception (Verh\"utungspille), see Stahel 7.3.j:\\[2mm]

\includegraphics[width=6cm]{graphics/table1.png}\\[2mm]
{\small ``Hormonal contraception'' is the predictor ($x$) and ``heart attack'' the outcome ($y$).}\\[2mm]

{\bf Question:} Does hormonal contraception ($x$) have an influence on heart attacks ($y$)?\\[2mm]

This question is \alert{equivalent to asking whether the proportion} of patients with heart attack \alert{is the same} in both groups.\\



}

\frame[containsverbatim]{
The respective test-statistic can be calculated as\\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{equation*}
T = \sum_{\text{all entries}} \frac{(\text{observed} - \text{expected})^2}{\text{expected}} \ .
\end{equation*}
\end{minipage}}

~\\[8mm]
By hand, $T$ is obtained as 
%
\begin{equation*}
\frac{(23 - 14.8)^2}{14.8} + \frac{(34 - 42.2)^2}{42.2} + \frac{(35 -  43.2)^2}{43.2} + \frac{(132 - 123.8)^2}{123.8} =  8.329
\end{equation*}

~\\[2mm]
and is expected to be $\chi^2_{1}$ distributed {\tiny(one degree of freedom: $(2-1)\cdot(2-1)$)}. \\[3mm]

The $p$-value of this test is given as $\Pr(X\geq 8.329)=\Sexpr{format(1-pchisq(8.329,1),4,4,4)}$. \\[2mm]

<<>>=
1-pchisq(8.329,1)
@
}

\frame[containsverbatim]{
Of course, R can do this for us:\\[2mm]

<<chisq1>>=
contYes <- c(23,34)
contNo <- c(35,132)
data.table <- data.frame(rbind(contYes,contNo))
chisq.test(data.table,correct=FALSE)
@

<<echo=F>>=
#names( data.table) <- c("heartYes","heartNo")
n <- sum(c(contYes,contNo))
TT <- (23 - 58*57/n)^2 /(58*57/n) + (34 - 166*57/n)^2/(166*57/n) + (35 - 58*167/n)^2/(58*167/n) + (132 - 166*167/n)^2/ (166*167/n)
@
~\\[8mm]

Please note: You would usually prefer to have \texttt{correct=TRUE} to obtain a better approximation to the $\chi^2$ distribution (continuity correction of Yates):\\[2mm]

<<chisq2>>=
chisq.test(data.table,correct=TRUE)
@


~\\[4mm]
{\bf In any case, there is \alert{strong evidence} for an association of hormonal contraception with heart attacks!}
}

%\frame{\frametitle{Fisher's exact test}}

\frame{\frametitle{Quantification of a dependency}
{\tiny(Stahel GLM chapter 7.4)}\\[2mm]

If two variables are not independent, it is often desired to \alert{quantify} the dependency.\\[2mm]

Let one variable be the grouping variable (e.g., hormonal contraception vs no hormonal contraception). Then $\pi_1$ and $\pi_2$ are the relative frequencies (proportions) observed in the two groups. For example:

\begin{eqnarray*}
\pi_1 = 23/57 &=& 0.404 \\
\pi_2 = 35/167 &=& 0.210\\
\end{eqnarray*}

are the proportions of females with a heart attack in the two groups.
}

\frame{

There are at least three numbers that can be calculated to quantify how the two groups differ:\\[2mm]

\begin{itemize}
\item Risk difference: $\pi_1 - \pi_2 = 0.404 - 0.210$ = 0.194\\[4mm]

\item Relative risk: $\pi_1 / \pi_2 = 0.404 / 0.210 = 1.92 $\\[4mm]
\item \alert{Odds ratio} (``Chancenverh\"altnis''):
\begin{equation*}
OR = \frac{\pi_1 / (1-\pi_1)}{\pi_2 / (1-\pi_2)} = \frac{ 0.404 / (1- 0.404)}{0.210 / (1-0.210)} = 2.55 \ ,
\end{equation*}

where $\pi/(1-\pi)$ is the chance (die ``Chance'').\\[4mm]
Interpretation:
\begin{enumerate}
\item   $OR=1$  $\rightarrow$ the two groups are independent.
\item   $OR > 1$ $\rightarrow$ positive dependency.
\item   $OR < 1$ $\rightarrow$ negative dependency.
\end{enumerate}
\end{itemize}
}


\frame{\frametitle{The odds and the odds ratio}
\begin{itemize}
\item The {\bf odds} (``Wetteverh\"altnis''): For a probability $\pi$ the odds is $\pi/(1-\pi)$. For example, if the probability to win a game is 0.75, then the odds is given as 0.75/0.25 or 3:1.\\[4mm]

\item The {\bf odds ratio} is given on the previous slide. It is a ratio of two ratios, or, the {\bf ratio of two odds}.\\[4mm]

\item Often the {\bf log odds ratio} is used:
\begin{equation*}
\log(OR) = \log\left(\frac{\pi_1 / (1-\pi_1)}{\pi_2 / (1-\pi_2)} \right) \ .
\end{equation*}

Why is this simpler? Look at the interpretation:\\[2mm]
\begin{enumerate}
\item   $\log(OR)=0$ $\rightarrow$ the two groups are independent.
\item   $\log(OR)>0$ $\rightarrow$ positive dependency.
\item   $\log(OR)<0$ $\rightarrow$ negative dependency.
\end{enumerate}

\end{itemize}
}


\frame{\frametitle{Binomial and binary regression}
Usually the situation is more complicated than 
\begin{center}{\bf binary covariate} ($\bm{x}$) $\rightarrow$ {\bf binary outcome} ($\bm{y}$)\\[10mm]\end{center}

Often, we are interested in a relationship 
\begin{center}  {\bf Continuous/categ./binary} variables $\bm{x}^{(1)}$, $\bm{x}^{(2)}$,.. $\rightarrow$ {\bf binary outcome} ($\bm{y}$) \\[10mm]
\end{center} 

$\rightarrow$ A regression model is needed again!


}

\frame[containsverbatim]{\frametitle{Illustrative/working example}
Let us look at an example from chapter 9.2 in Hector (2015): \\[2mm]

Eight groups of beetles were exposed to carbon disulphide (an insecticide) for 5h. For each beetle it was then reported if it was killed or not (1 or 0), but the data were reported aggregated:\\[6mm]

<<echo=F>>=
library(AICcmodavg)
data(beetle)
beetle
@

~\\[6mm]
{\bf Question:} (How) does the insecticide affect the survival of the beetles?
}


\frame[containsverbatim]{\frametitle{}
As always, start with a graph:

\begin{center}
\setkeys{Gin}{width=0.6\textwidth}
<<fig=T,width=3.5,height=2.5,echo=F>>=
ggplot(beetle, aes(x = Dose, y = Mortality_rate)) + theme_bw() +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
geom_smooth(span = 1, colour = "red", se = FALSE) +
xlab("Dose") + ylab("Mortality rate")
@
\end{center}

with linear (blue) and smoothed line (red).
}


\frame{\frametitle{}
{\bf What can we see from the plot?}
\begin{itemize}
\item Mortality increases with higher doses of the herbicide (not surprising, right?).\\[2mm]
\item The linear line seems unreasonable. In particular, if one extrapolates to lower or higher doses, mortality would become $<0$ or $>1$, which is not possible. {\tiny (Remember: A probability is between 0 and 1 by definition.)}\\[8mm]
\end{itemize}

{\bf How does one analyze these data correctly?}
\begin{itemize}
\item So far, we know linear and Poisson regression.\\[2mm]
\item Both of these are \alert{not} the correct approaches here.\\[2mm]
\end{itemize}
}

\frame[containsverbatim]{\frametitle{The `wrong' analyses}
{\bf Linear regression}\\[2mm]

We could simply use 
$$\E(y_i) = \beta_0 + \beta_1 Dose_i $$
with $\E(y_i)=\pi_i =$ probability to die for individuals $i$ with $Dose_i$. \\[2mm]

R does this analysis without complaining (!!):\\ 

<<echo=T, eval=F>>=
lm(Mortality_rate ~ Dose, data=beetle)
@

<<echo=F, eval=T>>=
r.lm <- lm(Mortality_rate ~ Dose, data=beetle)
@

~\\[2mm]
This leads to $\hat\beta_0=\Sexpr{format(r.lm$coef[1],2,2,2)}$ and $\hat\beta_1=\Sexpr{format(r.lm$coef[2],2,2,2)}$. This means for instance that, for a zero dose, the probability to die would be $\E(y_i)=\Sexpr{format(r.lm$coef[1],2,2,2)}$.

~\\[4mm]
\colorbox{lightgray}{\begin{minipage}{10cm}
{\bf Problem:} Linear regression leads to impossible predicted probability values! $\Rightarrow$ \alert{Unrealistic predictions!}
\end{minipage}}
}



\frame[containsverbatim]{\frametitle{} \label{sl:poisson}
{\bf Poisson regression}\\[2mm]

What about Poisson regression with the counts ``\texttt{Number\_killed}'' in the response? We could use

$$\log(\E(y_i)) = \beta_0 + \beta_1 Dose_i$$

with $\E(y_i)=$ number killed. Again, R does this analysis without complaining (!!):\\ 

<<echo=T, eval=F>>=
glm(Number_killed ~ Dose, data=beetle,family=poisson)
@

<<echo=F, eval=T>>=
r.pois <- glm(Number_killed ~ Dose, data=beetle,family=poisson)
@

~\\[2mm]
This leads to $\hat\beta_0=\Sexpr{format(r.pois$coef[1],2,2,2)}$ and $\hat\beta_1=\Sexpr{format(r.pois$coef[2],2,2,2)}$. \\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
{\bf Problem:} This means for instance that, for a dose of 76, one expects that $\E(y_i)=\exp(\hat\beta_0 + \hat\beta_1 \cdot 76) = 
\Sexpr{format(exp(r.pois$coef[1] + r.pois$coef[2]*76),2,2,2)}$ beetles die. However, there are only around 60 beetles in each group, so the predicted number killed is more than what is available. $\Rightarrow$ \alert{Unrealistic predictions!}
\end{minipage}}
}

\frame{\frametitle{A model for binary data?}
I hope you remember the Bernoulli distribution from Mat183:\\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
The probability distribution of a binary random variable $Y$ $\in \{0,1\}$ with parameter $\pi$ is defined as
\begin{equation*}
\P(Y=1) = \pi \ , \quad  \P(Y=0) = 1-\pi \ .
\end{equation*}
\end{minipage}}

~\\[3mm]
{\bf Characteristics of the Beroulli distribution:}
\begin{itemize}
\item $\E(Y) = \pi$
\item $\Var(Y)=\pi(1-\pi)$.\\[2mm]

$\rightarrow$ The variance of the distribution is determined by its mean.\\[4mm]
\end{itemize}
}

\frame{\frametitle{Doing it right: Logistic regression}
We can again use the GLM machinery from last week! The \alert{linear predictor} is again (as always):

\begin{equation*}
\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ .
\end{equation*}

~\\
We again need a \alert{link function} that relates the linear predictor $\eta_i$ to the expected value $\E(y_i)$. \\[4mm]

Remember we used the $\log$ link last week, but that seems a bad idea here (see slide \ref{sl:poisson}).\\[4mm]

The link function must be chosen such that the expected value $\E(y_i)$ is always between 0 and 1!

}

\frame{\frametitle{Link function: The logistic transformation}

}



\frame[containsverbatim]{\frametitle{Your turn!}

<<>>=
path <- "../../data_examples/WBL/"
d.heart <- read.table(paste(path,"heart.dat",sep=""),header=T,sep=",")
@
}

\frame[containsverbatim]{
<<echo=F>>=
names(data.table)<- c("heartYes","heartNo")
data.table$cont <- c(1,0)
@

<<>>=
data.table
@

~\\[2mm]
<<>>=
r.heart.glm <- glm(cbind(heartYes,heartNo) ~ cont,data.table,family=binomial)
summary(r.heart.glm)
@


}


\frame{\frametitle{Summary}

}
% \frame{References:
% \bibliographystyle{Chicago}
% \bibliography{refs}
% }



\end{document}

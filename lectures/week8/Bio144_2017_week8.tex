\documentclass[english,9pt,aspectraio=169]{beamer}
\usepackage{etex}
\usetheme{uzhneu-en-informal}
%\usepackage{uarial}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\RequirePackage{graphicx,ae}
\usepackage{bm}
\usepackage{fancybox,amssymb,color}
\usepackage{pgfpages}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{animate}
\usepackage{numprint}
\usepackage{vwcol} 
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{amsmath,natbib}
\usepackage{mathbbol}
\usepackage{babel}
\usepackage{SweaveSlides}
\usepackage{multicol}
\usepackage{xcolor}


\usetheme{uzhneu-en-informal}
\DeclareMathOperator{\po}{Poisson}
\DeclareMathOperator{\G}{Gamma}
\DeclareMathOperator{\Be}{Beta}
\DeclareMathOperator{\logit}{logit}
\def\n{\mathop{\mathcal N}}

\definecolor{Gray}{RGB}{139,137,137}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{Green}{rgb}{0,0.8,0.3}
\definecolor{lightgreen}{rgb}{0,0.7,0.3}
\definecolor{Blue}{rgb}{0,0,1}
\def\myalert{\textcolor{darkred}}
\def\myref{\textcolor{Gray}}
\setbeamercovered{invisible}

\renewcommand{\baselinestretch}{1.2}
\beamertemplateballitem
\DeclareMathOperator{\cn}{cn} % Copy number
\DeclareMathOperator{\ccn}{ccn} % common copy number
\DeclareMathOperator{\p}{p} % common copy number
\DeclareMathOperator{\E}{E} % common copy number
\DeclareMathOperator{\given}{|} % common copy number
\def\given{\,|\,}
\def\na{\tt{NA}}
\def\nin{\noindent}
\pdfpageattr{/Group <</S /Transparency /I true /CS /DeviceRGB>>}
\def\eps{\varepsilon}

\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Wahrscheinlichkeitsma√ü
\def\eps{\varepsilon}
\def\logit{\text{logit}}
%\newcommand{\E}{\mathsf{E}} % Erwartungswert
\newcommand{\Var}{\text{Var}} % Varianz
\newcommand{\Cov}{\text{Cov}} % Varianz
\newcommand{\NBin}{\text{NBin}}
\newcommand{\Po}{\text{Po}}
\newcommand{\N}{\mathsf{N}}

\newcommand{\hl}{\textcolor{red}}

\newcommand{\ball}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{1ex}{1ex}
\usebeamercolor[fg]{item projected}

{\pgftransformscale{1.75}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%
\usepackage{multicol}
\newcommand{\ballsmall}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{.2ex}{.2ex}

{\pgftransformscale{1}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%



\begin{document}

\fboxsep5pt

\frame{
\title[]{ \centering \Huge Kurs Bio144: \\
Datenanalyse in der Biologie}%\\[.3cm]
\author[Stefanie Muff, Owen L.\ Petchey]{\centering Stefanie Muff  \& Owen L.\ Petchey }
%\institute[]{Institute of Social and Preventive Medicine \\ Institute of Evolutionary Biology and Environmental Studies}
\date[]{Week 8: Interpretation, causality, cautionary notes \\ 27./28. April 2017}


\maketitle
}


\frame{\frametitle{Overview (todo: check)}
\begin{itemize}
\item $P$-values: Interpretation and (mis-)use\\[2mm]
\item Statistical significance vs biological relevance\\[2mm]
\item Relative importance of regression terms\\[2mm]
\item Causality vs correlation\\[2mm]
\item Bradford-Hill criteria for causal inference\\[2mm]
\item Experimental vs observational studies\\[2mm]
\end{itemize}

 
}


\frame{\frametitle{Course material covered today}
\begin{itemize}
\item todo
\end{itemize}

\vspace{4mm}
\textcolor{blue}{\bf Optional reading:}
\begin{itemize}
\item todo
\end{itemize}
}

\frame[containsverbatim]{\frametitle{Recap of Last week}
\begin{itemize}
\item todo
\end{itemize}
}




 
\frame{\frametitle{$P$-values }
{\bf Recap:}\\

$P$-values are commonly used for \emph{statistical testing}, e.g.\ by checking if $p<0.05$.\\[4mm]


{\bf Examples:} \\

\begin{itemize}
\item $T$-test for a difference between two samples. \\[2mm]
\item $\chi^2$-test for independence of two discrete distributions. \\[2mm]
\item Test if a regression coefficient $\beta_x\neq 0$ in a regression model. 
\end{itemize}
}


\frame[containsverbatim]{\frametitle{$P$-values in regression models}

In regression modelling, the $p$-value is often used as an indicator of covariate importance. Remember the mercury example:


% latex table generated in R 3.3.2 by xtable 1.8-2 package
% Thu Jan 12 17:47:24 2017
\begin{table}[!h]
\centering
\begingroup\footnotesize
\begin{tabular}{rrrr}
  \hline
 & Coefficent & 95\%-confidence interval & $p$-value \\ 
  \hline
Intercept & -0.68 & from -0.88 to -0.47 & $<$ 0.0001 \\ 
  log10(Hg\_soil) & 0.033 & from -0.05 to 0.11 & 0.42 \\ 
  vegetables & 0.07 & from -0.03 to 0.17 & 0.18 \\ 
  migration & -0.036 & from -0.19 to 0.12 & 0.65 \\ 
  smoking & 0.27 & from 0.06 to 0.48 & 0.012 \\ 
  sqrt(amalgam) & 0.33 & from 0.24 to 0.42 & $<$ 0.0001 \\ 
  age & -0.042 & from -0.06 to -0.02 & 0.0004 \\ 
  mother & -1.03 & from -1.70 to -0.35 & 0.003 \\ 
  sqrt(fish) & 0.079 & from 0.03 to 0.13 & 0.004 \\ 
  last\_fish & 0.30 & from 0.15 to 0.45 & $<$ 0.0001 \\ 
  age:mother & 0.055 & from 0.03 to 0.08 & 0.0002 \\ 
   \hline
\end{tabular}
\endgroup
\end{table}
A common practice is to look only at the $p$-value and use $p<0.05$ to decide whether a variable has an influence or not.
}



\frame{\frametitle{$P$-values criticism}

$P$-value {\bf criticism is} as {\bf old} as statistical significance testing (1920s!). Issues:

\begin{itemize}
\item The sharp line $p<0.05$ is \alert{arbitrary} and significance testing according to it may lead to \emph{mindless statistics} \citep{gigerenzer2004}. \\[2mm]

\item Model selection using $p$-values may lead to a \alert{model selection bias} (see last week).\\[2mm]

\item  $P$-hacking / dada dredging: Search until you find a result with $p<0.05$.\\[2mm]

\item Publication bias: Studies with $p<0.05$ are more likely to be published than ``non-significant'' results.\\[2mm]


\item Recent articles in \emph{Science}, \emph{Nature} or a statement by the \emph{American Statistical Associaton (ASA)} in March 2016 show that the debate still continues \citep{claridge-chang.assam2016,goodman2016,wasserstein.lazar2016}.\\[4mm]

\end{itemize}
}

\frame{\frametitle{$P$-values even made it into NZZ (April 2016)}
 \includegraphics[width=10cm]{pictures/NZZ1.jpeg}
}


\frame{
Note: R.A. Fisher, the ``inventor'' of the $p$-value (1920s) didn't mean the $p$-value to be used in the way it is used today (which is: doing a single experiment and use $p<0.05$ for a conclusion)!\\[2mm]

From \citet{goodman2016}:\\[2mm]
\begin{quote}
Fisher used ``significance'' merely {\bf to indicate that an observation was worth following up, with refutation of the null hypothesis justified only if further experiments ``rarely failed'' to achieve significance.} 
This is in stark contrast to the modern practice of making claims based on a single demonstration of statistical significance.
\end{quote}

~\\[2mm]
\colorbox{lightgray}{\begin{minipage}{10cm}
The misuse of $p$-values has led to a \alert{reproducibility crisis} in science!
\end{minipage}}
}

\frame{

\includegraphics[width=10cm]{pictures/Ioannidis2.png}\\
{\tiny \citep{ioannidis2005}}
}

\frame{\frametitle{What is the problem with the $p$-value?}

Many applied researchers do not \alert{really} understand what the $p$-value actually is.\\[2mm]


\colorbox{lightgray}{\begin{minipage}{10cm}
The {\bf formal definition of $p$-value} is the probability of an observed data summary (e.g., an average) and its more extreme values, given a specified mathematical model and hypothesis (usually the ``Null'').
\end{minipage}}\\
{\scriptsize \citep{goodman2016}}


 
\setkeys{Gin}{width=1\textwidth}
\includegraphics{Bio144_2017_week8-pValFig}

} 
 
\frame{\frametitle{Klicker-Exercise}


\href{http://www.klicker.uzh.ch/bkx}
{\beamergotobutton{Klicker-Exercise}}

\url{http://www.klicker.uzh.ch/bkx}
\vspace{5mm}

+ Discussion of the results!
% For each of the following interpretations, decide if it is right or wrong:
% \begin{itemize}
% \item The $p$-value is the probability that the null hypothesis is true. (no)
% \item The $p$-value is the probability that the observed data occurred by chance.(no)
% \item $(1-p)$ is the probability that the alternative hypothesis is true.(no)
% \end{itemize}
}
 

\frame{\frametitle{Significance vs relevance}
In regression models:\\[2mm]
\begin{itemize}
\item A low $p$-value does not automatically imply that a variable is ``important''.
\item ``Is there an effect?'' v.s. ''How much of an effect is there?''.\\[6mm]
\end{itemize}
\begin{center}
 \includegraphics[width=7cm]{pictures/pValueEffectSize.jpg}
 \end{center}
 {(\scriptsize from Goodman, 2008)}
}


\frame{\frametitle{Shall we abolish $p$-values?}

\textcolor{red}{\bf No:} $p$-values are not ``good'' or ``bad''. They contain important information, and they have have {\bf strengths} and {\bf weaknesses}. \\[12mm]


 

Suggestions:
\begin{itemize}
\item Use $p$-values, but don't over-interpret them, \alert{use them properly}.\\[2mm]
\item Look at \alert{effect sizes} and \alert{confidence intervals}.\\[2mm]
\item Look at \alert{relative importances} of covariates.\\[2mm]
\item {\bf Don't use $p$-values for model selection.} \\[2mm]
\end{itemize}

}


\frame{\frametitle{Suggestion 1: Proper interpretation of $p$-values}
Rather than a black-and-white decision ($p<0.05$), Martin Bland suggests to regard $p$-values as continuous measures for statistical evidence
(Introduction to Medical Statistics, 4th edition, Oxford University Press): \\[8mm]

\begin{tabular}{ll}
$p > 0.1$  & little or no evidence against the null hypothesis \\[2mm]
$0.1 > p > 0.05$&  weak evidence\\[2mm]
$0.05 > p > 0.01$ & evidence\\[2mm]
$0.01 > p > 0.001$ & strong evidence\\[2mm]
$p < 0.001 $ & very strong evidence\\[8mm]
\end{tabular}

\colorbox{lightgray}{\begin{minipage}{10cm}
But: The level of significance must also depend on the context!
\end{minipage}}
}


\frame[containsverbatim]{
In the Hg example:
% latex table generated in R 3.3.2 by xtable 1.8-2 package
% Thu Jan 12 17:47:25 2017
\begin{table}[!h]
\centering
\begingroup\footnotesize
\begin{tabular}{rrrr}
  \hline
 & Coefficent & 95\%-confidence interval & $p$-value \\ 
  \hline
Intercept & -0.68 & from -0.88 to -0.47 & $<$ 0.0001 \\ 
  log10(Hg\_soil) & 0.033 & from -0.05 to 0.11 & 0.42 \\ 
  vegetables & 0.07 & from -0.03 to 0.17 & 0.18 \\ 
  migration & -0.036 & from -0.19 to 0.12 & 0.65 \\ 
  smoking & 0.27 & from 0.06 to 0.48 & 0.012 \\ 
  sqrt(amalgam) & 0.33 & from 0.24 to 0.42 & $<$ 0.0001 \\ 
  age & -0.042 & from -0.06 to -0.02 & 0.0004 \\ 
  mother & -1.03 & from -1.70 to -0.35 & 0.003 \\ 
  sqrt(fish) & 0.079 & from 0.03 to 0.13 & 0.004 \\ 
  last\_fish & 0.30 & from 0.15 to 0.45 & $<$ 0.0001 \\ 
  age:mother & 0.055 & from 0.03 to 0.08 & 0.0002 \\ 
   \hline
\end{tabular}
\endgroup
\end{table}\begin{itemize}
\item {\bf Little or no evidence:} Hg soil, vegetables from garden, migration background\\
\item {\bf Weak evidence:} Smoking
\item {\bf Strong evidence:} Mother, monthly fish consumption
\item {\bf Very strong evidence:} Amalgam, age, last fish (> or < 3 days), interaction of age and mother
\end{itemize}
}


\frame{\frametitle{Suggestion 2: Report effect sizes.... }
\colorbox{lightgray}{\begin{minipage}{10cm}
Ask: {\bf Is the effect size \emph{relevant}?}
\end{minipage}}\\[8mm]

{\bf Example}\\[2mm] 
WHO recommendation concerning smoking and the consumption of processed meat. Both, smoking and meat consumption, appear to be carcinogenic.

\begin{itemize}
\item 50g processed meat per day increases the risk for colon cancer by a factor of 1.18 (+18\%).
\item Smoking increases the risk for cancer by a factor of 3.6 (+260\%).\\[6mm]
\end{itemize}

Thus: Although both, meat consumption and smoking, are carcinogenic (``significant''), their {\bf effect sizes are vastly different}!

}


\frame{\frametitle{...and 95\% CIs}
\colorbox{lightgray}{\begin{minipage}{10cm}
Ask: {\bf Which range of true effects is statistically consistent with the observed data?} 
\end{minipage}}\\[5mm]

{\bf Example}\\[2mm] 

Body fat example, slide 39 of week 1. \\[3mm]

The effect estimate for the effect of BMI on body fat is given as  $\hat\beta_{BMI} = 1.82$, 95\% CI from 1.61 to 2.03. \\[3mm]

{\bf Interpretation:} for an increase in the bmi by one index point, roughly 1.82\% percentage points more bodyfat are expected, and all true values for $\beta_{BMI}$ between 1.61 and 2.03 are {\bf compatible with the observed data}. \\[2mm]

}


\frame{\frametitle{However...}
\begin{itemize}
\item The choice of the \alert{95\% is again somewhat arbitrary}. We could also go for 90\% or 99\% or any other interval, but 95\% has established as a commonly accepted range.\\[9mm]
\item The 95\% CI should {\bf not be misused for simple hypothesis testing} in the sense of \\[2mm]
``Is 0 in the confidence interval or not?''\\[2mm]
Because this boils down to checking whether $p<0.05$ ...

\end{itemize}
}


\frame{\frametitle{Suggestion 3:  Look at relative importances of covariates}

\begin{itemize}
\item Ultimately, the popularity of $p$-values is based on the wish to judge which covariates are {\bf relevant}
in a model, particularly in observational studies.\\[4mm]

\item The problem with this: Low $p$-values do not automatically imply high relevance \citep{cox1982}.\\[4mm]

\item Alternative: {\bf relative importances} of explanatory variables that measure the proportion (\%) of the responses' variability explained by each variable.
\end{itemize}

}


\frame{\frametitle{Relative importance: Decomposing $R^2$}
{\bf Remember:} $R^2$ indicates the proportion of variance explained by {\bf all} covariates in a model
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_2 x_i^{(m)} + e_i   \ . 
\end{equation*}
~\\

\colorbox{lightgray}{\begin{minipage}{10cm}
The aim or relative importance is to \alert{decompose} $R^2$ such that 
\begin{itemize}
\item each variable $x^{(j)}$ is attributed a fair share $r_j$.\\[2mm]
\item the sum of all importances sums up to $R$, that is, $\sum_{j=1}^p r_j = R^2$.\\
\end{itemize}
\end{minipage}}

\vspace{6mm}

Further, it is required that
\begin{itemize}
\item all shares are $\geq 0$.
\end{itemize}
}


\frame{
	\frametitle{Question: How would you define/calculate relative importance?}

	\begin{itemize}
		\item {\bf Idea 1:} Fit simple models including only one covariate at the time, \emph{i.e.}:
		\begin{equation*}
y_i = \beta_0 + \beta_j x_i^{(j)} + e_i   
\end{equation*}
for each variable $x^{(j)}$ and use the respective $R^2$ as $r_j$. \\[6mm]

		\item {\bf Idea 2:} Fit  the linear model twice, once with and once without the covariate of interest, and then take the \myalert{increase} of $R^2$ as $r_j$.\\[1cm]
		
		
		% Compare $R^2$ from the full model to the $R_{-j}^2$ from the model without covariate $x^{(j)}$ and use the difference as $r_j$.\\[10mm]
\end{itemize}

Problem: In practice, regressors $x^{(j)}$ are \emph{always correlated}, thus both ideas lead to $\sum_j r_j \neq R^2$!

%This contribution is called \myalert{relative importance} or \myalert{\% of variance explained}. 

}

\frame{\frametitle{ }
To understand the problem of ideas 1 and 2, let us fit three models for $\log(Hg_{\text{urine}})$ with \begin{itemize}
\item$x^{(1)}=\sqrt{\text{Number of monthly fish meals}}$ 
\item $x^{(2)}=$ binary indicator if last fish meal was less than 3 days ago.
\end{itemize}
These two variables are correlated (people who consume a lot of fish are more likely to have it consumed within the last 3 days).\\[-10mm]


\vspace{8mm}
\begin{eqnarray} 
y_i = \beta_0 + \beta_{1}  x^{(1)}_i  +   + e_i   & \quad &  R^2 = 0.12\\
y_i = \beta_0 + \beta_{2}  x^{(2)}_i  +   + e_i   & \quad &  R^2 = 0.08\\
y_i = \beta_0 + \beta_{1}   x^{(1)}_i  + \beta_{2}  x^{(2)}_i    + e_i  & \quad & R^2 = 0.14 
\end{eqnarray}

\colorbox{lightgray}{\begin{minipage}{10cm}
{\bf Note:} The $R^2$ of the model with both covariates is much less than the sum of the $R^2$ from models (1) and (2)!
\end{minipage}}

~\\[2mm]
$\Rightarrow$ The increase of $R^2$ upon inclusion of a covariate depends on the covariates that are already in the model!


% {\bf Example} \\
% With two collinear covariates, only inclusion of the first increases $R^2$.
}


\frame{\frametitle{A better way to calculate relative importance?}

Various proposals to calculate relative importance ($R^2$ decomposition) have been proposed. The (currently) most useful is given by the following idea, called {\bf LMG} ({\bf L}indemann, {\bf M}erenda and {\bf G}old 1980):\\[4mm]

\begin{itemize}
\item Fit the model for \alert{all possible orderings of the covariates}.\\[2mm]
\item Record the incrase in $R^2$ each time a variable is included.\\[2mm]
\item \alert{Average} over all orderings of the covariates.\\[10mm]
\end{itemize}


{\bf Luckily, the {\tt R}-package \myalert{{\tt relaimpo}}  (Groemping 2006) contains the function {\tt calc.relimp()} that does this for us!}

}


\frame[containsverbatim]{\frametitle{Hg results}\label{sl:relimp}
Which proportion (\%) of variance in $\log(Hg_{\text{urine}})$ is explained by each covariate?\\[4mm]

\begin{Schunk}
\begin{Sinput}
> library(relaimpo)
> lmg.hg <- calc.relimp(r.lm.hg)$lmg
\end{Sinput}
\end{Schunk}

\begin{table}
\begin{tabular}{l @{\hspace{1cm}} r @{\hspace{1cm}} r}
Variable & Rel.\ imp.\ (\%)  & $p$-value\\
\hline
$\log(Hg_{\text{soil}})$ & 0.10  & 0.42\\
Vegetable & 0.46 & 0.18\\
Migration & 0.43 & 0.65\\
Smoking & 1.21 & 0.012\\
Amalgam & 19.69 & <0.0001 \\
Age & 1.25 & 0.0004\\
Mother & 1.08 & 0.0031\\
Fish & 7.26 & 0.0042\\
Last fish & 7.34 & <0.0001\\
Age:mother & 6.56 & 0.0002\\
\end{tabular}
\end{table}
}

\frame{
Several variables have very low $p$-values, but their relative importance differs clearly.\\[4mm]

$\Rightarrow$ Relative importance gives intuitive \alert{complementary information} to $p$-values, effect sizes and confidence intervals!
}


\frame{\frametitle{Does relative importance solve all the problems?}
Unfortunately not... \\[2mm]

Relative importance should be understood as \alert{a complement to standard statistical output}. \\[6mm]

There are several limitations to it:
\begin{itemize}
\item Rel.imp.\ of a variable may heavily depend on the other variables included in the model, especially when there are strongly correlated variables (see next slide).\\[2mm]


\item Hard to generalize to other, non-linear regression models.
\end{itemize}
}

\frame{
Groemping 2007:\\[3mm]
``...a request for a decomposition of $R^2$ is often driven by a desire to prioritize intervention actions with the intention to influence the response. It is important to notice that \myalert{any intervention bears the risk [...] of not only influencing the targeted regressor but also the correlation structure among regressors.} Thus, unexpected results may occur regarding changes of the response's variance. In this way, the benefit of the concept of decomposing $R^2$ is more limited than the typical user might realize.''
}


\frame[containsverbatim]{\frametitle{Example}
Compare the estimated relative importance for the variable \texttt{fish} (monthly fish meals) for two cases:\\[2mm]

{\bf Model 1}\\
Original Hg model. \\[2mm]

{\bf Model 2}\\
Model \alert{without the indicator variable \texttt{last\_fish}}.

 

~\\[4mm]

\begin{itemize}
\item {\bf Case 1:} Relative importance of \texttt{fish}: 7.26\% (see slide \ref{sl:relimp}).\\[2mm]
\item {\bf Case 2:} Relative importance of \texttt{fish}: $10.75$\% .\\[8mm] 
\end{itemize}

{\bf Interpretation:} If one of two correlated variables is removed, the other absorbs some of the importance from it.
}


\frame{
Ev give another example where rel. imp is calculated, e.g. from previous weeks.

}


\frame{\frametitle{Causality vs correlation}

In explanatory models the ultimate goal usually is to reveal \alert{causal relationships} between the covariates and the response.\\[2mm]

{\bf Examples:}
\begin{itemize}
\item Does Hg in the soil influence Hg-levels in humans?\\[2mm]
\item Does inbreeding negatively affect population growth of Swiss Alpine ibex (Steinbock)?\\[2mm]
\item Does exposure to Asbest lead to illness or death?\\[2mm]
\item ...\\[6mm]
\end{itemize}

{\bf However:}
Regression models actually only reveal associations, that is, \alert{correlations} between $\bm{x}$ and $\bm{y}$!
}




\frame[allowframebreaks]{\frametitle{Bradford-Hill-Criteria for causal inference}
In 1965 the Epidemiologist Bradford Hill presented a list of criteria to assess whether there is some causality or not. However, he wrote ``None of my nine viewpoints can bring indisputable evidence for or against the cause-and-effect hypothesis and none can be required sina qua non.''\\[4mm]

{\bf Bradford-Hill Criteria:}
\begin{enumerate}
\item {\bf Strength:} A causal relationship is likely when the observed association is strong.
\item {\bf Consistency:} A causal relationship is likely if mutiple independent studies show similar associations.
\item {\bf Specificity:} A causal relationship is likely when a covariate $x$ is associated only with one potential outcome $y$ and not with other outcomes.
\item {\bf Temporality:} The effect has to occur after the cause.
\item {\bf Biological gradient:}
\item {\bf Plausibility:}
\item {\bf Coherence:}
\item {\bf Analogy:}
\item {\bf Experiment:}
\end{enumerate}
}


\frame{\frametitle{Causality considerations for model selection}
 

It is {\bf widely unknown} that a model can be broken by the inclusion of a ``wrong'' covariate, which is causally associated in the wrong direction:\\[4mm]

\begin{center}
\includegraphics[width=8cm]{pictures/causality.jpg}
\end{center}
~\\[2mm]

{\bf Remember:} Avoid to include covariates in your model that are \alert{caused} by the outcome!\\[4mm]

{\bf Example:} ...

}


\frame{\frametitle{Experimental vs observational studies}}


\frame{\frametitle{Summary}

}
\frame{References:
\bibliographystyle{Chicago}
\bibliography{refs}
}



\end{document}

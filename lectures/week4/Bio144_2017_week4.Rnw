\documentclass[english,9pt,aspectraio=169]{beamer}
\usepackage{etex}
\usetheme{uzhneu-en-informal}
%\usepackage{uarial}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\RequirePackage{graphicx,ae}
\usepackage{bm}
\usepackage{fancybox,amssymb,color}
\usepackage{pgfpages}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{animate}
\usepackage{numprint}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{amsmath,natbib}
\usepackage{mathbbol}
\usepackage{babel}
\usepackage{SweaveSlides}
\usepackage{multicol}
\usepackage{xcolor}


\usetheme{uzhneu-en-informal}
\DeclareMathOperator{\po}{Poisson}
\DeclareMathOperator{\G}{Gamma}
\DeclareMathOperator{\Be}{Beta}
\DeclareMathOperator{\logit}{logit}
\def\n{\mathop{\mathcal N}}

\definecolor{Gray}{RGB}{139,137,137}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{Green}{rgb}{0,0.8,0.3}
\definecolor{Blue}{rgb}{0,0,1}
\def\myalert{\textcolor{darkred}}
\def\myref{\textcolor{Gray}}
\setbeamercovered{invisible}

\renewcommand{\baselinestretch}{1.2}
\beamertemplateballitem
\DeclareMathOperator{\cn}{cn} % Copy number
\DeclareMathOperator{\ccn}{ccn} % common copy number
\DeclareMathOperator{\p}{p} % common copy number
\DeclareMathOperator{\E}{E} % common copy number
\DeclareMathOperator{\given}{|} % common copy number
\def\given{\,|\,}
\def\na{\tt{NA}}
\def\nin{\noindent}
\pdfpageattr{/Group <</S /Transparency /I true /CS /DeviceRGB>>}
\def\eps{\varepsilon}

\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Wahrscheinlichkeitsma√ü
\def\eps{\varepsilon}
\def\logit{\text{logit}}
%\newcommand{\E}{\mathsf{E}} % Erwartungswert
\newcommand{\Var}{\text{Var}} % Varianz
\newcommand{\NBin}{\text{NBin}}
\newcommand{\Po}{\text{Po}}
\newcommand{\N}{\mathsf{N}}

\newcommand{\ball}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{1ex}{1ex}
\usebeamercolor[fg]{item projected}

{\pgftransformscale{1.75}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%
\usepackage{multicol}
\newcommand{\ballsmall}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{.2ex}{.2ex}

{\pgftransformscale{1}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%




\begin{document}
\SweaveOpts{width=6,height=4}
\fboxsep5pt

\frame{
\title[]{ \centering \Huge Kurs Bio144: \\
Datenanalyse in der Biologie}%\\[.3cm]
\author[Stefanie Muff, Owen L.\ Petchey]{\centering Stefanie Muff  \& Owen L.\ Petchey }
%\institute[]{Institute of Social and Preventive Medicine \\ Institute of Evolutionary Biology and Environmental Studies}
\date[]{Week 4: Multiple linear regression (finalize) / Residual analysis / Checking modelling assumptions\\ 16./17. March 2017}


\maketitle
}


\frame{\frametitle{Overview (todo: check)}
\begin{itemize}
\item Interactions between covariates
\item Multiple vs.\ many single regressions
\item Checking assumptions / Model validation 
\item What to do when things go wrong?
\item Transformation of variables/the response
\item Handling of outliers
\end{itemize}
}


\frame{\frametitle{Course material covered today}
\begin{itemize}
\item Chapter 3.3 in \emph{Lineare Regression}
\item To do
\end{itemize}
}

\frame[containsverbatim]{\frametitle{Recap of last week I}
to do 
}

\frame[containsverbatim]{\frametitle{Recap of last week I}
Last week we introduced binary and factor covariates that allowed for group-specific intercepts.
}


\frame[containsverbatim]{\frametitle{Group-specific slopes / Interactions}
It may happen that groups do not only differ in their intercept ($\beta_0$), but also in their slopes ($\beta_x$).\\[2mm]

For simplicity, let us look at a binary covariate ($x_i \in \{0,1\}$). 
}



\frame[containsverbatim]{\frametitle{}

Remember the mercury (Hg) example from last week. We now extended the dataset and include mothers \emph{and} children ($\leq 11$ years).\\[2mm]

It is known that Hg concentrations may change over the lifetime of humans. So let us look at \texttt{log(Hg$_\text{urin}$)} depending on the participants age:
\vspace{-5mm}
\begin{center}
\setkeys{Gin}{width=0.55\textwidth}
<<fig=T,echo=F,width=4.5,height=4>>=
path <- "~/Teaching/Bio144/data_examples/Hg/"
d.hg <- read.table(paste(path,"Hg_urin.csv",sep=""),header=T, sep=",")
d.hg <- d.hg[,c(1,2,5,6,7,8,10)]
names(d.hg) <- c("Hg_urin", "Hg_soil", "smoking","amalgam", "age", "fish","mother")
plot(log(Hg_urin) ~ age, d.hg,col=mother+1,cex.lab=1.5)
legend("topright",legend=c("children","mothers"),col=1:2,pch=1)
abline(lm(log(Hg_urin)~age,data=subset(d.hg,mother==0)))
abline(lm(log(Hg_urin)~age,data=subset(d.hg,mother==1)),col=2)
@
\end{center}
An important observation is that children and mothers show different dependencies of age!
}

\frame{
It is therefore crucial to formulate a model that allows for different intercepts \emph{and} slopes, depending on group membership (mother/child).\\[2mm]

The smallest possible model is then given as\\
\begin{equation}\label{eq:HgInt}
y_i =  \beta_0 + \beta_1 \text{mother}_i + \beta_2 \text{age}_i + \beta_3\text{age}_i\cdot \text{mother}_i + e_i \ , 
\end{equation}
~\\
where $y_i=\log(Hg_{\text{urin}})_i$, and \texttt{mother} is a binary ``dummy'' variable that indicates if the person is a mother (1) or a child (0).\\[4mm]

This results in essentially {\bf two} models with group specific intercept and slope:\\[2mm]
\colorbox{lightgray}{\begin{minipage}{10cm}
Mothers ($x_i=1$): $\hat{y}_i = \beta_0 +  \beta_1 + (\beta_2 +\beta_3)\text{age}_i + e_i$  \\[2mm]
Children ($x_i=0$): $\hat{y}_i = \beta_0  + \beta_2 \text{age}_i + e_i$  
\end{minipage}}
}




\frame[containsverbatim]{\frametitle{}
Fitting model \eqref{eq:HgInt} in R is done as follows, where \texttt{age:mother} denotes the interaction term ($\text{age}_i\cdot \text{mother}_i$):\\[6mm]

<<echo=F>>=
path <- "~/Teaching/Bio144/data_examples/Hg/"
d.hg <- read.table(paste(path,"Hg_urin.csv",sep=""),header=T, sep=",")
d.hg <- d.hg[,c(1,2,5,6,7,8,10)]
names(d.hg) <- c("Hg_urin", "Hg_soil", "smoking","amalgam", "age", "fish","mother")
@
<<echo=T>>=
r.hg <- lm(log(Hg_urin)~  mother + age + age:mother,d.hg)
summary(r.hg)$coef
@
~\\[4mm]

Interpretation: \\[2mm]

Mothers: $\hat{y}_i = \Sexpr{round(r.hg$coefficients[1],2)} + (\Sexpr{round(r.hg$coefficients[2],2)}) + (\Sexpr{round(r.hg$coefficients[3],2)} + \Sexpr{round(r.hg$coefficients[4],2)}) \cdot \text{age}_i$ \\[2mm]
Children: $\hat{y}_i = \Sexpr{round(r.hg$coefficients[1],2)}  + (\Sexpr{round(r.hg$coefficients[3],2)}) \cdot \text{age}$ \\[2mm]

\begin{itemize}
\item The Hg level drops in young children.
\item The Hg level increases in adults (mothers).
\end{itemize}
}

\frame[containsverbatim]{\frametitle{}
Remember (from last week), however, that the Hg model also included smoking status, amalgam fillings and fish consumption as important predictors. It is very straightforward to just include these predictors in model \eqref{eq:HgInt}, which leads to the following model \\[6mm]

<<echo=T>>=
r.hg <- lm(log(Hg_urin)~  mother * age + smoking + amalgam + fish,d.hg)
@

<<results=tex,echo=F>>=
library(biostatUZH)
tableRegression(r.hg)
@

{\small (Note that \texttt{mother*age} in R encodes for \texttt{mother} + \texttt{age} + \texttt{mother:age}.)}
}

\frame[containsverbatim]{
Again, for completeness, some model checking: 

\setkeys{Gin}{width=0.8\textwidth}

<<modelChecksHg,fig=T,width=7,height=4,echo=F>>=
par(mfrow=c(1,2))
plot(r.hg$fitted,r.hg$residuals,xlab="Fitted", ylab="Residuals")
abline(h=0,lty=2)
qqnorm(r.hg$residuals)
qqline(r.hg$residuals)
@
}

\frame{\frametitle{Multiple vs.\ many single regressions}
Question: I find group-specific intercepts and interactions too complicated. Could I simply fit separate models for each variable?\\[6mm]

\pause
Answer (Stahel 3.3o):\\[4mm]
\includegraphics[width=11cm]{pictures/citation.pdf}
~\\[4mm]

Why?
}

\frame{\frametitle{Illustration}
Chapter 3.3c in the Stahel script illustrates the point on four artificial examples. The model is always given as 
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + e_i \  ,
\end{equation*}

where $\bm{x}^{(1)}$ is a continuous variable, and $\bm{x}^{(2)}$ is a binary grouping variable (thus taking values 0 for group 0 and 1 for group 1).\\[4mm]

Thus the model is 
\begin{equation*}
\begin{array}{lll}
\hat{y_i} &= \beta_0 +  \beta_1 x_i^{(1)} & \text{if $x_i^{(2)}=0$.} \\
\hat{y_i} &= \beta_0 + \beta_2 + \beta_1 x_i^{(1)}   & \text{if $x_i^{(2)}=1$.} 
\end{array}
\end{equation*}
}

\frame{
\includegraphics[width=11cm]{pictures/FigsAB.jpg}\\[2mm]

Example A: Within-group slope is $>0$. Fitting $y\sim x$ leads to an overestimated slope when group-variable is not included in the model.\\[4mm]

Example B: Within-group slope is $0$, but fitting $y\sim x$ leads to a slope estimate $>0$, wich is only an artefact of not accounting for the group variable $x^{(2)}$. \\[4mm]
}

\frame{
\includegraphics[width=11cm]{pictures/FigsCD.pdf}\\[2mm]

Example C: Within-group slope is $<0$, but fitting $y\sim x$ leads to an estimated slope of $>0$! \\[4mm]

Example D: Within-group slope is $<0$, but fitting $y\sim x$ leads to a slope estimate of $0$. \\[4mm]
}


\frame{\frametitle{Another interpretation of multiple regression}
In multiple regression, the coefficient $\beta_x$ of a covariate $x$ can be interpreted as follows:\\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
$\beta_x$ explains how the response changes with $x$, while holding all the other variables constant.
\end{minipage}}

~\\[2mm]
This idea is similar in spirit to an experimental design, where the influence of a covariate of interest on the response is investigated in various environments\footnote{Clayton, D. and M. Hills (1993). Statistical Models in Epidemiology. Oxford: Oxford University Press.}. Clayton and Hills (1993) continue (p.273):\\[3mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{quote}
To extend our analogy, the data analyst is in a position like that of an experimental scientist who has the capability to plan and carry out many experiments within a single day. Not surprisingly, a cool head is required!
\end{quote}
\end{minipage}}
}

\frame{\frametitle{Checking modelling assumptions}
Remember from week 2, that in linear regression the modelling assumption is that the residuals $e_i$ are independently normally distributed around zero, that is, $e_i \sim \N(0,\sigma_e^2)$. This implies four things:\\[2mm]
\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{enumerate}[a)]
\item The expected value of $e_i$ is 0: $\E(e_i)=0$.\\[2mm]
\item All $e_i$ have the same variance: $\Var(e_i)=\sigma_e^2$. \\[2mm]
\item The $e_i$ are normally distributed.\\[2mm]
\item The $e_i$ are independent of each other.
\end{enumerate}
\end{minipage}}
~\\[2mm]
So far, we have discussed the Tukey-Anscombe plot and the QQ-plot.
}

\frame{
Stahel 4.1b:
\includegraphics[width=11cm]{pictures/41b.pdf}
~\\[6mm]

The aim is to find a model that describes the data well. But always keep in mind the following statement from a wise man:\\[6mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
All models are wrong, but some are useful.\\
\scriptsize{(Box 1978)}
\end{minipage}}
}

\frame{\frametitle{Overview of model-checking tools}

Those used in this course are:\\[4mm]

\begin{itemize}
\item Tukey-Anscombe plot (see weeks 2 and 3)\\
$\Rightarrow$ \myalert{To check assumptions a), b) and d)}\\[2mm]

\item Quantile-quantile (QQ) plot (see weeks 2 and 3)\\
$\Rightarrow$ \myalert{To check assumption c)}\\[2mm]


\item Scale-location plot (Streuungs-Diagramm)\\
$\Rightarrow$ \myalert{To check assumption b)}\\[2mm]

\item Leverage plot (Hebelarm-Diagramm)\\
$\Rightarrow$ \myalert{To find influencial observations}
\end{itemize}
}

\frame[containsverbatim]{\frametitle{Tukey-Anscombe plot}
<<echo=F>>=
path <- "~/Teaching/Bio144/data_examples/Hg/"
d.hg <- read.table(paste(path,"Hg_urin.csv",sep=""),header=T, sep=",")
d.hg <- d.hg[d.hg$mother==1,-c(3,4,9,10,11)]
names(d.hg) <- c("Hg_urin", "Hg_soil", "smoking","amalgam", "age", "fish")
@


It is useful to enrich the TA-plot by adding a ``running mean'' or a ``smoothed mean'', which can give hints on the trend of the residuals. For the mercury example where $\log(Hg_{\text{urin}})$ is regressed on smoking, amalgam and fish consumption (slides 33-35 or week 3): 
\vspace{-10mm}

\begin{multicols}{2}{ll}
% \begin{center}
\setkeys{Gin}{width=0.5\textwidth}
<<hgFig,fig=T,echo=F,width=4,height=4>>=
r1.urin.mother <- lm(log10(Hg_urin) ~  smoking + amalgam + fish,data=d.hg)
plot(fitted(r1.urin.mother),residuals(r1.urin.mother),xlab="fitted",ylab="residual")
abline(h=0,lty=2)
aa <- data.frame(cbind(x=fitted(r1.urin.mother), y=residuals(r1.urin.mother)))
aa <- aa[order(aa$x),]
lo <- loess(y ~ x, aa)
lines(aa$x,predict(lo), col='red', lwd=2,lty=5)
@
\vspace{5mm}
~\\[1cm]
% \end{center}
The TA plot (again) indicates that there is a small problem in the range of -0.7 to -0.6, namely due to an outlier...\\
\end{multicols}

}


\frame{
We claimed that the TA plot is also able to check the \emph{independence assumption} d). But how?\\[4mm]

A dependency would be reflected by some kind of \myalert{trend}. \\[4mm]

Other ideas to plot residuals to check for a dependency? Please discuss!

}

% Todo: hide the following slide in the published version of the slides!!
\frame{
The dependency is not necessarily on the fitted values ($x$-axis of TA plot). Ideas: 
\begin{itemize}
\item Plot in dependency of time (if available) or sequence of obervations.
\item Plot against the covariates.
\end{itemize}
\begin{center}
\setkeys{Gin}{width=0.9\textwidth}
<<hgFigCov,fig=T,echo=F,width=7,height=4>>=
par(mfrow=c(1,2))
plot(d.hg$amalgam,residuals(r1.urin.mother),xlab="No of amalgam",ylab="residual")
abline(h=0,lty=2)
plot(d.hg$fish,residuals(r1.urin.mother),xlab="No of fish",ylab="residual")
abline(h=0,lty=2)
@
\end{center}
}

\frame{\frametitle{QQ-plot}

The \myalert{outlier} recorded above is also visible in the (well-known) QQ-plot, which is useful to check normal distribution of residuals (assumption c):
\begin{center}
\setkeys{Gin}{width=0.6\textwidth}
<<hgFigQQ,fig=T,echo=F,width=4,height=4>>=
qqnorm(residuals(r1.urin.mother))
qqline(residuals(r1.urin.mother))
@
\end{center}
}


\frame{\frametitle{Scale-location plot (Streuungs-Diagramm)}}


\frame{\frametitle{Leverage plot}

}



\frame[containsverbatim]{\frametitle{Outliers}
<<echo=F>>=
path <- "~/Teaching/Bio144/data_examples/Hg/"
d.hg <- read.table(paste(path,"Hg_urin.csv",sep=""),header=T, sep=",")
d.hg <- d.hg[d.hg$mother==1,-c(3,4,9,10,11)]
names(d.hg) <- c("Hg_urin", "Hg_soil", "smoking","amalgam", "age", "fish")
@
<<r1urin,echo=F>>=
r1.urin.mother <- lm(log10(Hg_urin) ~  smoking  + amalgam + fish,data=d.hg)
#summary(r1.urin.mother)
@
\setkeys{Gin}{width=0.6\textwidth}
<<fig=T,width=4,height=4>>=
plot(fitted(r1.urin.mother),residuals(r1.urin.mother))
abline(h=0,lty=2)
qqnorm(fitted(r1.urin.mother))
qqline(fitted(r1.urin.mother))
@

}
% \frame{References:
% \bibliographystyle{Chicago}
% \bibliography{refs}
% }



\end{document}
